rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
rf_grid_perf
rf_grid.h2o
# Random forest H2O
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# Script with alternative machine learning regression models
#     - Random Forest
#     - Gradient Boosting Model
#     - XGBoost
#     - Support Vector Machine
# import required packages
library(MASS) #includes databases
library(randomForest) #Random Forest model
library(gbm) #gradient boosting model
library(xgboost) #xgboost model
library(e1071) #support vector machine model
library(h2o) #grid search and optimization
set.seed(23122) #set seed to keep consistent results
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
# Separating data
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
# Random Forest Model Regression
rf <- randomForest(medv ~ ., data = Boston[train,])
rf_mse <- mean((test_y_data - predict(rf, test_x_data)) ^ 2)
#### Gradient Boosting Model Regression #####
#basic GBM
gbm_boost <- gbm(medv ~ . ,data = Boston[train,],distribution = "gaussian",n.trees = 10000,
shrinkage = 0.01, interaction.depth = 4)
#summary(gbm_boost) #gives a table of Variable Importance and a plot of Variable Importance
gbm_mse <- mean((test_y_data - predict(gbm_boost, as.data.frame(test_x_data))) ^ 2)
# XGBoost Model using Tree Boosting
xgb_tree <- xgboost(data = train_x_data, label = train_y_data,
booster = "gbtree", #use decision trees
nrounds = 1000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
max_depth = 6,  #maximum depth of the tree
eta = .25,  #determines learning rate. Lower: less overfitting, but more nrounds and slower computation
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_tree_train_mse <- mean((test_y_data - predict(xgb_tree, test_x_data)) ^ 2)
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 0, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 0, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
# Support Vector Machine regression
svm_model <- svm(medv ~ ., data = Boston[train, ])
svm_mse <- mean((test_y_data - predict(svm_model, test_x_data)) ^ 2)
#### Using h2o ####
h2o.no_progress()
h2o.init(max_mem_size = "8g")  #start up h2o
# Random forest H2O
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
# Random forest H2O
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
# Random forest H2O
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = as.h2o(Boston[train,]),
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model)
training_data <- Boston[train, ]
training_data <- as.h2o(Boston[train, ])
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
# Random forest H2O
training_data <- as.h2o(Boston[train, ])
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
h2o.no_progress()
h2o.init(max_mem_size = "8g")  #start up h2o
# Random forest H2O
training_data <- as.h2o(Boston[train, ])
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric of choice
rf_grid_perf <- h2o.getGrid(
grid_id = "rf_grid",
sort_by = "mse",
decreasing = FALSE
)
# Grab the model_id for the top model, chosen by validation error
rf_best_model_id <- rf_grid_perf@model_ids[[1]]
rf_best_model <- h2o.getModel(rf_best_model_id)
# Now let’s evaluate the model performance on a test set
rf_best_model_perf <- h2o.performance(model = rf_best_model, newdata = as.h2o(Boston[-train,]))
training_data <- as.h2o(Boston[train, ])
num_p <- length(Boston[train,]) - 1
rf_grid.h2o <- list(
ntrees      = c(200, 500, 1500),
mtries      = c(round(num_p /2), round(num_p / 3)),
max_depth   = c(20, 30, 40),
sample_rate = c(.55, .632, .75)
)
# random grid search criteria
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 30*60
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
# build grid search
rf_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_grid",
y = "medv",
training_frame = training_data,
hyper_params = rf_grid.h2o,
search_criteria = search_criteria
)
?h2o.grid
# GDM H2O
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.5, .75, 1),
col_sample_rate = c(.8, .9, 1)
)
gdm_search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# perform grid search for gdm
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid1",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = hyper_grid,
searcg_criteria = gdm_search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# collect the results and sort by our model performance metric of choice
gdm_grid_perf <- h2o.getGrid(
grid_id = "gbm_grid1",
sort_by = "mse",
decreasing = FALSE
)
# GDM H2O
# create hyperparameter grid
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.5, .75, 1),
col_sample_rate = c(.8, .9, 1)
)
gdm_search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# perform grid search for gdm
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid1",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = hyper_grid,
search_criteria = gdm_search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# collect the results and sort by our model performance metric of choice
gdm_grid_perf <- h2o.getGrid(
grid_id = "gbm_grid1",
sort_by = "mse",
decreasing = FALSE
)
#grid_perf
# Grab the model_id for the top model, chosen by validation error
gdm_best_model_id <- gdm_grid_perf@model_ids[[1]]
gdm_best_model <- h2o.getModel(gdm_best_model_id)
# performance metrics on the best model
h2o.performance(model = gdm_best_model, valid = TRUE, newdata = as.h2o(Boston[-train,]))
gdm_grid_perf
hyper_grid <- list(
max_depth = c(1, 3, 5),
min_rows = c(1, 5, 10),
learn_rate = c(0.01, 0.05, 0.1),
learn_rate_annealing = c(.99, 1),
sample_rate = c(.5, .75, 1),
col_sample_rate = c(.8, .9, 1)
)
gdm_search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.005,
stopping_rounds = 10,
max_runtime_secs = 60*60
)
# perform grid search for gdm
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid1",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = hyper_grid,
search_criteria = gdm_search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# perform grid search for gdm
grid <- h2o.grid(
algorithm = "gbm",
grid_id = "gbm_grid1",
y = "medv",
training_frame = as.h2o(Boston[train, ]),
validation_frame = as.h2o(Boston[-train,]),
hyper_params = hyper_grid,
#search_criteria = gdm_search_criteria,
ntrees = 5000,
stopping_rounds = 10,
stopping_tolerance = 0,
seed = 123
)
# collect the results and sort by our model performance metric of choice
gdm_grid_perf <- h2o.getGrid(
grid_id = "gbm_grid1",
sort_by = "mse",
decreasing = FALSE
)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
?h2o.grid
