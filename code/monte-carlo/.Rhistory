# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, y2, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", "y2", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) -
(1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
y2 <- x %*% beta
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, y2, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", "y2", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) - (1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
print(r)
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.9, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) - (1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
print(r)
x <- cbind(1, rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
as.matrix = TRUE
))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99, 0.99, 1, 0, 0.99, 0, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99, 0.99, 1, 0.1, 0.99, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99,
0.99, 1, 0.1,
0.99, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.9,
0.9, 1, 0.1,
0.9, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1), var = c(5, 5, 5))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = 0.5)
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = 0.9)
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "symmetric", corr = 0.9)
?distance
source('~/github/reu-2021/code/variable-selection/monte-carlo.r', echo=TRUE)
source('~/github/reu-2021/code/variable-selection/monte-carlo.r', echo=TRUE)
source('~/github/reu-2021/code/variable-selection/monte-carlo.r', echo=TRUE)
source('~/github/reu-2021/code/variable-selection/monte-carlo.r', echo=TRUE)
View(results)
results$soln - results$lasso
(results$soln - results$lasso)^2
sum((results$soln - results$lasso)^2)
sqrt(sum((results$soln - results$lasso)^2))
source('~/github/reu-2021/code/monte-carlo/run.r', echo=TRUE)
?source
setwd("~/github/reu-2021/code/monte-carlo")
source('~/github/reu-2021/code/monte-carlo/run.r', echo=TRUE)
View(dat)
View(dat[["coefficients"]])
dat <- monte_carlo(n = 100, p = 11, seed = 1)
dat <- monte_carlo(n = 100, p = 3, seed = 1)
source('~/github/reu-2021/code/monte-carlo/run.r', echo=TRUE)
dat <- monte_carlo(n = 100, p = 3, seed = 1)
View(dat)
View(dat[["coefficients"]])
dat <- monte_carlo(n = 100, p = 10, seed = 1)
source('~/github/reu-2021/code/monte-carlo/simulation.r', echo=TRUE)
dat <- monte_carlo(n = 100, p = 10, seed = 1)
View(dat)
View(dat[["coefficients"]])
dat <- monte_carlo(n = 100, p = 3, seed = 1)
View(dat[["coefficients"]])
dat <- monte_carlo(n = 100, p = 15, seed = 1)
View(dat[["coefficients"]])
dat <- monte_carlo(n = 1000, p = 15, seed = 1)
View(dat[["coefficients"]])
?MASS
??MASS
?replicate
license90
license()
RShowDoc("GPL-3")
source('~/NSFREU/reu-2021/code/monte-carlo/run.r', echo=TRUE)
dat <- monte_carlo(n = 25, p = 100, iterations = 5, beta = c(1, 1, 1), error_var = 0.2)
source('~/NSFREU/reu-2021/code/monte-carlo/run.r', echo=TRUE)
source('~/NSFREU/reu-2021/code/monte-carlo/run.r', echo=TRUE)
View(dat)
View(dat[[1]][["confusion"]][["soln"]])
install.packages("caret")
?caret
library(caret)
?caret
?confusionMatrix
library(MASS)
Boston
size(Boston)
length(Boston)
nrows(Boston)
rows(Boston)
nrow(Boston)
install.packages("gbm")
install.packages("xgboost")
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
.75*nrow(Boston)
round
round(.75*nrow(Boston))
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
Boston.boost
summary(Boston.boost)
coef(Boston.boost)
coefs(Boston.boost)
??gbm
?gbm
utils::browseVignettes("gbm")
browseVignettes("gbm"
browseVignettes("gbm"
browseVignettes("gbm")
browseVignettes("gbm")
utils::browseVignettes("gbm")
library(utils)
utils::browseVignettes("gbm")
?xgboost
header(Boston)
head(Boston)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
?gbm
# XGBoost Model Technique
xgb_model <- xgboost(medv ~ ., data = Boston[train,])
?xgboost
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, -"medv"], label = Boston[train, "medv"])
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, !"medv"], label = Boston[train, "medv"])
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, .-"medv"], label = Boston[train, "medv"])
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, "medv"], label = Boston[train, 1:length(Boston)-1])
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, "medv"], label = Boston[train, 1:length(Boston)-1])
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, "medv"], label = Boston[train, 1:length(Boston)-1], max_depth = 2, eta = 1, nthread = 2, nrounds = 2,
objective = "binary:logistic")
# XGBoost Model Technique
xgb_model <- xgb.DMatrix(data = Boston[train, "medv"], label = Boston[train, 1:length(Boston)-1])
# XGBoost Model Technique
xgb_model <- xgb.DMatrix(data = matrix(Boston[train, "medv"]), label = matrix(Boston[train, 1:length(Boston)-1]))
# XGBoost Model Technique
xgb_model <- xgb.DMatrix(data = matrix(Boston[train, 1:length(Boston)-1]]), label = matrix(Boston[train, "medv"]))
# XGBoost Model Technique
xgb_model <- xgb.DMatrix(data = matrix(Boston[train, 1:length(Boston)-1]), label = matrix(Boston[train, "medv"]))
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
View(Boston)
?Boston
# XGBoost Model Technique
xgb_model <- xgboost(data = matrix(Boston[train, 1:length(Boston)-1]), label = matrix(Boston[train, "medv"]), max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
# XGBoost Model Technique
xgb_model <- xgboost(data = Boston[train, 1:length(Boston)-1], label = Boston[train, "medv"], max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
matrix(Boston[train, 1:length(Boston)-1])
Boston[train, 1:length(Boston)-1]
Boston[train, "medv"]
matrix(Boston[train, "medv"])
Boston[train, 1:length(Boston)-1]
test <-  Boston[train, 1:length(Boston)-1]
matrix(test)
as.matrix(Boston[train, 1:length(Boston)-1])
# XGBoost Model Technique
xgb_model <- xgboost(data = as.matrix(Boston[train, 1:length(Boston)-1]), label = matrix(Boston[train, "medv"]), max.depth = 2, eta = 1, nthread = 2, nrounds = 2, objective = "binary:logistic")
# XGBoost Model Technique
xgb_model <- xgboost(data = as.matrix(Boston[train, 1:length(Boston)-1]), label = matrix(Boston[train, "medv"]), nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25)
# XGBoost Model Technique
xgb_model <- xgboost(data = as.matrix(Boston[train, 1:length(Boston)-1]), label = matrix(Boston[train, "medv"]), nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25)
View(xgb_model)
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])
train_y_data <- as. matrix(Boston[train, "medv"])
xgb_model <- xgboost(data = train_x_data, label = train_y_data, nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25)
# XGBoost Model Technique
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])
train_y_data <- as.matrix(Boston[train, "medv"])
xgb_model <- xgboost(data = train_x_data, label = train_y_data, nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25)
?xgboost
params(xgb_model)
View(xgb_model)
xgb_model <- xgboost(data = train_x_data, label = train_y_data, nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25,
verbose = 0)
xgb_model <- xgboost(data = train_x_data, label = train_y_data, nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25,
verbose = 2)
xgb_model <- xgboost(data = train_x_data, label = train_y_data, nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25,
verbose = 0)
xgb_model <- xgboost(data = train_x_data, label = train_y_data,
#nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25,  #determines learning rate. Lower: less overfitting, but more nrounds and longer computation cost
verbose = 0)
xgb_model <- xgboost(data = train_x_data, label = train_y_data,
nrounds = 1000,
objective = "reg:squarederror",
early_stopping_rounds = 3,
max_depth = 6,
eta = .25,  #determines learning rate. Lower: less overfitting, but more nrounds and longer computation cost
verbose = 0)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
View(xgb_tree)
View(xgb_linear)
coef(xgb_linear)
coefs(xgb_linear)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
# XGBoost Model Technique using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
verbose = 1)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
# XGBoost Model Technique using Tree Boosting
xgb_tree <- xgboost(data = train_x_data, label = train_y_data,
booster = "gbtree", #use devision trees
nrounds = 1000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
max_depth = 6,  #maximum depth of the tree
eta = .25,  #determines learning rate. Lower: less overfitting, but more nrounds and slower computation
verbose = 1)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
install.packages("randomForest")
?randomForest
?forest
library(randrandomForest) #Random Forest model
library(randomForest) #Random Forest model
?randomForest
# Random Forest Model
rf <- randomForest(medv ~ ., data = Boston[train,])
View(rf)
summary(randomForest)
summary(rf)
rf
rf$mse
mean(rf$mse)
rf$mse[-1]
rf$mse[:-1]
rf$mse[-1:]
rf$mse[:1]
rf$mse[:]
rf$mse[2:]
rf$mse[2:12]
predict(xgb_tree, as.matrix(Boston[! train, 1:length(Boston)-1]))
?predict
?predict.xgb.Booster
as.matrix(Boston[! train, 1:length(Boston)-1])
Boston[! train, 1:length(Boston)-1]
Boston[!train, 1:length(Boston)-1]
Boston[train, 1:length(Boston)-1]
train
Boston[-train, 1:length(Boston)-1]
predict(xgb_tree, as.matrix(Boston[-train, 1:length(Boston)-1]))
as.matrix(Boston[-train, "medv"])
Boston[-train, "medv"] - predict(xgb_tree, as.matrix(Boston[-train, 1:length(Boston)-1]))
(Boston[-train, "medv"] - predict(xgb_tree, as.matrix(Boston[-train, 1:length(Boston)-1]))) ^ 2
mean((Boston[-train, "medv"] - predict(xgb_tree, as.matrix(Boston[-train, 1:length(Boston)-1]))) ^ 2)
xgb_tree_train_mse <- mean((Boston[-train, "medv"] - predict(xgb_tree, as.matrix(Boston[-train, 1:length(Boston)-1]))) ^ 2)
xgb_tree$best_score
xgb_linear_train_mse <- mean((Boston[-train, "medv"] - predict(xgb_linear, as.matrix(Boston[-train, 1:length(Boston)-1]))) ^ 2)
?predict.randomForest
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
xgb_tree_train_mse <- mean((test_y_data - predict(xgb_tree, test_x_data)) ^ 2)
predict(rf, test_x_data)
rf_mse <- mean((test_y_data - predict(rf, test_x_data)) ^ 2)
?xgboost
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 0, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 0, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 0, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 1, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 1, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 0, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 0.5, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 0.5, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
# XGBoost Model using Linear Boosting
xgb_linear <- xgboost(data = train_x_data, label = train_y_data,
booster = "gblinear", #use linear models
nrounds = 10000,  #number of boosting iterations
objective = "reg:squarederror",  #regression by RSS
early_stopping_rounds = 3,  #stops boosting early if mse doesnt improve after a certain number of rounds
lambda = 0, #L2 regularization term on weights (ridge regression tuning parameter)
alpha = 0, #L1 regularization term on weights (lasso regression tuning parameter)
verbose = 0)  #whether to print information during boosting: 0 = nothing, 1 = some info, 2 = more info
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
xgb_linear_train_mse <- mean((test_y_data - predict(xgb_linear, test_x_data)) ^ 2)
install.packages("e1071")
library(e1071) #support vector machine model
?svm
svm_model <- svm(medv ~ ., data = Boston[train, ])
View(svm_model)
svm_mse <- mean((test_y_data - predict(svm_model, test_x_data)) ^ 2)
gbm_mse <- mean((test_y_data - predict(gbm_boost, test_x_data)) ^ 2)
predict(gbm_boost, test_x_data)
gbm_mse <- mean((test_y_data - predict(gbm_boost, as.data.frame(test_x_data)) ^ 2)
gbm_mse <- mean((test_y_data - predict(gbm_boost, as.data.frame(test_x_data))) ^ 2)
gbm_mse <- mean((test_y_data - predict(gbm_boost, as.data.frame(test_x_data))) ^ 2)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
