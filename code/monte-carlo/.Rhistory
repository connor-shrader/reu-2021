library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# import required packages
library(MASS) #includes databases
library(randomForest) #Random Forest model
library(ranger) #Faster version of randomForestr
library(gbm) #gradient boosting model
library(xgboost) #xgboost model
library(e1071) #support vector machine model
library(h2o) #grid search and optimization
library(tidyverse)
set.seed(23122) #set seed to keep consistent results
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
######Separating data#####
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
##XGBoost Grid Search##
xgb_hyper_grid <- expand.grid(
eta = c(.01, .1, .3),  #learning rate
max_depth = c(1, 3, 7),
min_child_weight = c(3),
subsample = c(.8),
colsample_bytree = c(.9),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
# grid search
for(i in 1:nrow(xgb_hyper_grid)) {
# create parameter list
params <- list(
eta = xgb_hyper_grid$eta[i],
max_depth = xgb_hyper_grid$max_depth[i],
min_child_weight = xgb_hyper_grid$min_child_weight[i],
subsample = xgb_hyper_grid$subsample[i],
colsample_bytree = xgb_hyper_grid$colsample_bytree[i]
)
# reproducibility
set.seed(123)
# train model
xgb.tune <- xgb.cv(
params = params,
data = train_x_data,
label = train_y_data,
nrounds = 5000,
nfold = 5,
objective = "reg:squarederror",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# add min training error and trees to grid
xgb_hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
xgb_hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
}
xgb_best_grid <- xgb_hyper_grid %>%
dplyr::arrange(min_RMSE)
xgb_best_params <- list(
eta = xgb_best_grid$eta[1],
max_depth = xgb_best_grid$max_depth[1],
min_child_weight = xgb_best_grid$min_child_weight[1],
subsample = xgb_best_grid$subsample[1],
colsample_bytree = xgb_best_grid$colsample_bytree[1]
)
# train best model
xgb.best <- xgboost(
params = xgb_best_params,
data = train_x_data,
label = train_y_data,
nrounds = 5000,
objective = "reg:squarederror",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
predict(xgboost, test_x_data)
predict(xgb.best, test_x_data)
predict(xgb.best, new_data = test_x_data)
predict(xgb.best, newdata = test_x_data)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
predict(xgb.best, newdata = as.matrix(test_x_data))
predict(xgb.best, newdata = test_x_data)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_best_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
predict(best_rf_model, test_x_data)
predict(best_rf_model, test_x_data)$predictions
class(test_x_data)
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
# import required packages
library(MASS) #includes databases
library(randomForest) #Random Forest model
library(ranger) #Faster version of randomForestr
library(gbm) #gradient boosting model
library(xgboost) #xgboost model
library(e1071) #support vector machine model
library(h2o) #grid search and optimization
library(tidyverse)
set.seed(23122) #set seed to keep consistent results
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
######Separating data#####
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_best_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
best_rf_model
class(best_rf_model)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
View(res)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
View(res)
dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
)
View(dat)
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
library(rstudioapi)
library(h2o)
library(bit64)
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
)
View(ex_y_hat)
?corr
?corresp
corresp(ex_y_hat, ex_y_dat)
cor(ex_y_hat, ex_y_dat)
ex_y_dat[20]
ex_y_dat[18]
ex_y_dat[21]
ex_y_dat[11]
ex_y_dat[13]
ex_y_dat[12]
ex_y_dat[87]
?ranger
dat <- monte_carlo(n = 100,
p = 10,
type = "symmetric",
corr = 0.5,
sd = 1,
iterations = 1,
seed = 1
)
View(dat)
dat <- monte_carlo(n = 500,
p = 20,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
)
View(dat)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
# Used to set the current working directory to this script.
library(rstudioapi)
library(bit64)
# Used for XGBoost models.
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 5,
seed = 1
)
View(dat)
system.time(dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
))
res <- monte_carlo(n = 100, p = 10, iterations = 5, type = "independent", corr = 1)
source('~/NSFREU/reu-2021/code/monte-carlo/run.r', echo=TRUE)
system.time(dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
))
system.time(dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
))
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(MASS) # v7.3-54
library(caret)
# Used to set the current working directory to this script.
library(rstudioapi)
library(bit64)
# Used for XGBoost models.
library(xgboost)
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
View(dat)
system.time(dat <- monte_carlo(n = 100,
p = 10,
type = "independent",
corr = 0,
sd = 1,
iterations = 1,
seed = 1
))
View(dat)
R.version
install.packages("installr")
library(installr)
updateR()
R.version
R.version
