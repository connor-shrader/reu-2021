(1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = corr
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
y2 <- x %*% beta
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, y2, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", "y2", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 5, seed = 1, error_var = 1, type = "autoregressive", corr = 0.9)
generate_data(n = 5, p = 5, seed = 1, error_var = 1, type = "autoregressive", corr = 0.8)
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "autoregressive", corr = 0.8)
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
?multi_rnorm
?rnorm_multi
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) -
(1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = corr,
corr_mat = corr
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
y2 <- x %*% beta
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, y2, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", "y2", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) -
(1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
y2 <- x %*% beta
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, y2, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", "y2", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) - (1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
print(r)
x <- cbind(1, data.matrix(rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
)))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 10, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 1, 1, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.9, 1))
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.1, 1))
generate_data <- function(n, p, seed, var = 1, type = "independent", corr = 0,
beta = NULL, error_var = 1) {
set.seed(seed)
# Generate coefficient values.
if (is.null(beta)) {
beta <- c(1, 2, -2, 0, 0, 0.5, 3)
}
print("T")
if (length(beta) < p + 1) {
zeroes <- rep(0, (p + 1) - length(beta))
beta <- c(beta, zeroes)
}
else if (length(beta) > p + 1) {
beta <- beta[1:(p + 1)]
}
sd <- rep(sqrt(var), length.out = p)
if (type == "independent") {
r <- 0
}
else if (type == "symmetric") {
r <- corr
}
else if (type == "autoregressive") {
# Source for this code:
# https://statisticaloddsandends.wordpress.com/2020/02/07/generating-correlation-matrix-for-ar1-model/
exponent <- abs(matrix(1:n - 1, nrow = n, ncol = n, byrow = TRUE) - (1:n - 1))
r <- corr^exponent
print(r)
}
else if (type == "unstructured") {
r <- corr
}
print(r)
x <- cbind(1, rnorm_multi(
n = n,
vars = p,
mu = 0,
sd = sd,
r = r,
as.matrix = TRUE
))
# Generate corresponding y values.
y <- x %*% beta + rnorm(n, sd = sqrt(error_var))
# Create return data frame. We removed the column of 1's that we used as
# an intercept for generating the data.
dat <- data.frame(cbind(y, x[, -1]))
# Set the column names to "y, x1, x2, ..., xp"
colnames(dat) <- c("y", paste("x", 1:p, sep=""))
return(dat)
}
generate_data(n = 5, p = 2, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99, 0.99, 1, 0, 0.99, 0, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99, 0.99, 1, 0.1, 0.99, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.99, 0.99,
0.99, 1, 0.1,
0.99, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.9, 0.9,
0.9, 1, 0.1,
0.9, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1), var = c(5, 5, 5))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "unstructured", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = c(1, 0.5, 0.5,
0.5, 1, 0.1,
0.5, 0.1, 1))
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = 0.5)
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "independent", corr = 0.9)
generate_data(n = 5, p = 3, seed = 1, error_var = 1, type = "symmetric", corr = 0.9)
install.packages("gcdnet")
?gcdnet
?cv.gcdney
?cv.gcdnet
library(gcdnet  )
library(gcdnet)
?gcdnet
?cv.gcdnet
College
# Contains the College dataset.
library(ISLR)
# Plots will be generated in a 1 * 2 grid.
par(mfrow = c(1, 2))
# The leaps library contains functions for best subset selection,
# forward stepwise selection and backward stepwise selection.
library(leaps)
library(gcdnet) #library for adaptive elastic net regression
College
Boston
#setup
library(MASS) #used for Boston dataframe
library(gcdnet) #used for adaptive elastic net regression
library(tidyverse) #data cleaning and analysis
set.seed(23122) #set seed to keep consistent results
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
######Separating data#####
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
Boston
adap_enet <- cv.gcdnet(x = train_x_data, y = train_y_data, nfolds = 10)
adap_enet <- cv.gcdnet(x = train_x_data, y = train_y_data, nfolds = 10, method = "ls")
View(adap_enet)
y_hat <- predict(adap_enet, test_x_data)
?predict.cv.gcdnet
y_hat <- predict(adap_enet, newx = test_x_data)
mse <- mean((y_hat - test_y_data)^2)
library(glmnet) #used for lasso regression
lasso <- cv.glmnet(x = train_x_data, y = train_y_data, alpha = 1)
lasso_yhat <- predict(lasso, newx = test_x_data)
lasso_mse <- mean((lasso_yhat - test_y_data)^2)
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
# Used for stepwise selection.
library(MASS) # v7.3-54
# Used for confusion matrices.
library(caret) # v6.0-88
# Used to set the current working directory to this script.
library(rstudioapi) # v0.13
library(bit64) # v4.0.5
# Used for XGBoost models.
library(xgboost) # v1.4.1.1
# Used for random forest models.
library(ranger) # v0.12.1
# Support vector machine model
library(e1071) # v1.7-7
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
#random_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#models <- fit_models(dat = random_data, n = 100, p = 10)
#test_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#lasso_pred <- predict(models$lasso, as.matrix(test_data[, -1]))
#lasso_eval <- data.frame(lasso_pred, test_data[, 1])
#lasso_mse <- 1/100 * sum((lasso_eval[, 1] - lasso_eval[, 2])^2)
#rf_pred <- h2o.performance(models$rf, newdata = as.h2o(test_data))
#rf_pred <- as.data.frame(predict(object = models$rf, newdata = as.h2o(test_data[, -1])))
#rf_eval <- data.frame(rf_pred, test_data[, 1])
#rf_mse <- 1/100 * sum((rf_eval[, 1] - rf_eval[, 2])^2)
#rf_eval[3] <- lasso_eval[1]
#colnames(rf_eval) <- c("rf", "true", "lasso")
system.time(dat <- monte_carlo(n = 1000,
p = 100,
type = "symmetric",
corr = 0.5,
sd = 1,
iterations = 1,
seed = 1
))
# monte-carlo.r
# Gabe Ackall, Seongtae Kim, Connor Shrader
# This file contains everything to be executed using the functions from
# simulation.r and metrics.r.
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
# Used for stepwise selection.
library(MASS) # v7.3-54
# Used for confusion matrices.
library(caret) # v6.0-88
# Used to set the current working directory to this script.
library(rstudioapi) # v0.13
library(bit64) # v4.0.5
# Used for XGBoost models.
library(xgboost) # v1.4.1.1
# Used for random forest models.
library(ranger) # v0.12.1
# Support vector machine model
library(e1071) # v1.7-7
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
#random_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#models <- fit_models(dat = random_data, n = 100, p = 10)
#test_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#lasso_pred <- predict(models$lasso, as.matrix(test_data[, -1]))
#lasso_eval <- data.frame(lasso_pred, test_data[, 1])
#lasso_mse <- 1/100 * sum((lasso_eval[, 1] - lasso_eval[, 2])^2)
#rf_pred <- h2o.performance(models$rf, newdata = as.h2o(test_data))
#rf_pred <- as.data.frame(predict(object = models$rf, newdata = as.h2o(test_data[, -1])))
#rf_eval <- data.frame(rf_pred, test_data[, 1])
#rf_mse <- 1/100 * sum((rf_eval[, 1] - rf_eval[, 2])^2)
#rf_eval[3] <- lasso_eval[1]
#colnames(rf_eval) <- c("rf", "true", "lasso")
system.time(dat <- monte_carlo(n = 300,
p = 10,
type = "symmetric",
corr = 0.5,
sd = 1,
iterations = 1,
seed = 1
))
View(dat)
# monte-carlo.r
# Gabe Ackall, Seongtae Kim, Connor Shrader
# This file contains everything to be executed using the functions from
# simulation.r and metrics.r.
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
# Used for stepwise selection.
library(MASS) # v7.3-54
# Used for confusion matrices.
library(caret) # v6.0-88
# Used to set the current working directory to this script.
library(rstudioapi) # v0.13
library(bit64) # v4.0.5
# Used for XGBoost models.
library(xgboost) # v1.4.1.1
# Used for random forest models.
library(ranger) # v0.12.1
# Support vector machine model
library(e1071) # v1.7-7
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
#random_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#models <- fit_models(dat = random_data, n = 100, p = 10)
#test_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#lasso_pred <- predict(models$lasso, as.matrix(test_data[, -1]))
#lasso_eval <- data.frame(lasso_pred, test_data[, 1])
#lasso_mse <- 1/100 * sum((lasso_eval[, 1] - lasso_eval[, 2])^2)
#rf_pred <- h2o.performance(models$rf, newdata = as.h2o(test_data))
#rf_pred <- as.data.frame(predict(object = models$rf, newdata = as.h2o(test_data[, -1])))
#rf_eval <- data.frame(rf_pred, test_data[, 1])
#rf_mse <- 1/100 * sum((rf_eval[, 1] - rf_eval[, 2])^2)
#rf_eval[3] <- lasso_eval[1]
#colnames(rf_eval) <- c("rf", "true", "lasso")
system.time(dat <- monte_carlo(n = 300,
p = 10,
type = "symmetric",
corr = 0.5,
sd = 1,
iterations = 1,
seed = 1
))
View(dat)
#setup
library(MASS) #used for Boston dataframe
library(gcdnet) #used for adaptive elastic net regression
library(glmnet) #used for lasso regression
library(tidyverse) #data cleaning and analysis
set.seed(23122) #set seed to keep consistent results
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
######Separating data#####
train_x_data <-  as.matrix(Boston[train, 1:length(Boston)-1])  #training x data
train_y_data <- as.matrix(Boston[train, "medv"])  #training y data (AKA labels)
test_x_data <- as.matrix(Boston[-train, 1:length(Boston)-1])
test_y_data <- Boston[-train, "medv"]
adap_enet <- cv.gcdnet(x = train_x_data, y = train_y_data, nfolds = 10, method = "ls")
adap_enet_yhat <- predict(adap_enet, newx = test_x_data)
adap_enet_mse <- mean((adap_enet_yhat - test_y_data)^2)
time <- system.time(lasso <- cv.glmnet(x = train_x_data, y = train_y_data, alpha = 1))
lasso_yhat <- predict(lasso, newx = test_x_data)
lasso_mse <- mean((lasso_yhat - test_y_data)^2)
time
time[1]
time$user
time[2]
time[3]
?system.time
# monte-carlo.r
# Gabe Ackall, Seongtae Kim, Connor Shrader
# This file contains everything to be executed using the functions from
# simulation.r and metrics.r.
# R version: 4.1.0
library(tidyverse) # v1.3.1
library(dplyr) # v1.0.6
library(faux) # v1.0.0
library(ncvreg) # v3.13.0
library(glmnet) # v4.1-1
library(gcdnet) #v1.0.5
# Used for stepwise selection.
library(MASS) # v7.3-54
# Used for confusion matrices.
library(caret) # v6.0-88
# Used to set the current working directory to this script.
library(rstudioapi) # v0.13
library(bit64) # v4.0.5
# Used for XGBoost models.
library(xgboost) # v1.4.1.1
# Used for random forest models.
library(ranger) # v0.12.1
# Support vector machine model
library(e1071) # v1.7-7
setwd(dirname(getActiveDocumentContext()$path))
source("simulation.r")
source("metrics.r")
#random_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#models <- fit_models(dat = random_data, n = 100, p = 10)
#test_data <- generate_data(seed = 1, n = 100, p = 10, corr = 0, type = "independent")
#lasso_pred <- predict(models$lasso, as.matrix(test_data[, -1]))
#lasso_eval <- data.frame(lasso_pred, test_data[, 1])
#lasso_mse <- 1/100 * sum((lasso_eval[, 1] - lasso_eval[, 2])^2)
#rf_pred <- h2o.performance(models$rf, newdata = as.h2o(test_data))
#rf_pred <- as.data.frame(predict(object = models$rf, newdata = as.h2o(test_data[, -1])))
#rf_eval <- data.frame(rf_pred, test_data[, 1])
#rf_mse <- 1/100 * sum((rf_eval[, 1] - rf_eval[, 2])^2)
#rf_eval[3] <- lasso_eval[1]
#colnames(rf_eval) <- c("rf", "true", "lasso")
system.time(dat <- monte_carlo(n = 50,
p = 100,
type = "symmetric",
corr = 0.5,
sd = 1,
iterations = 1,
seed = 1
))
View(dat)
