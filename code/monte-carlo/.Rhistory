data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed
best_rf_model$predictions
best_rf_model$predictions
best_rf_model$prediction.error
# import required packages
library(MASS) #includes databases
library(randomForest) #Random Forest model
library(ranger) #Faster version of randomForestr
library(gbm) #gradient boosting model
library(xgboost) #xgboost model
library(e1071) #support vector machine model
library(h2o) #grid search and optimization
library(tidyverse)
nrows <- nrow(Boston)
train <- sample(1:nrow(Boston),size=round(nrows*.75))  #seperate 75% into training data
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
View(rf_best_grid)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
best_rf_model$predictions
best_rf_model$prediction.error
Boston
View(rf_hyper_grid)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i]#,
#seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed
)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
View(rf_best_grid)
View(best_rf_model)
View(best_rf_model)
View(rf_best_grid)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
View(best_rf_model)
View(rf_best_grid)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_best_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
View(rf_best_grid)
View(best_rf_model)
# import required packages
library(MASS) #includes databases
source('~/NSFREU/reu-2021/code/alternative-machine-learning/alternative-models.R', echo=TRUE)
##XGBoost Grid Search##
xgb_hyper_grid <- expand.grid(
eta = c(.01, .1, .3),  #learning rate
max_depth = c(1, 3, 7),
min_child_weight = c(3),
subsample = c(.8),
colsample_bytree = c(.9),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
# grid search
for(i in 1:nrow(xgb_hyper_grid)) {
# create parameter list
params <- list(
eta = xgb_hyper_grid$eta[i],
max_depth = xgb_hyper_grid$max_depth[i],
min_child_weight = xgb_hyper_grid$min_child_weight[i],
subsample = xgb_hyper_grid$subsample[i],
colsample_bytree = xgb_hyper_grid$colsample_bytree[i]
)
# reproducibility
set.seed(123)
# train model
xgb.tune <- xgb.cv(
params = params,
data = train_x_data,
label = train_y_data,
nrounds = 5000,
nfold = 5,
objective = "reg:squarederror",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
# add min training error and trees to grid
xgb_hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
xgb_hyper_grid$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
}
xgb_best_grid <- xgb_hyper_grid %>%
dplyr::arrange(min_RMSE)
xgb_best_params <- list(
eta = xgb_best_grid$eta[1],
max_depth = xgb_best_grid$max_depth[1],
min_child_weight = xgb_best_grid$min_child_weight[1],
subsample = xgb_best_grid$subsample[1],
colsample_bytree = xgb_best_grid$colsample_bytree[1]
)
# train best model
xgb.best <- xgb.cv(
params = xgb_best_params,
data = train_x_data,
label = train_y_data,
nrounds = 5000,
nfold = 5,
objective = "reg:squarederror",  # for regression models
verbose = 0,               # silent,
early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
##Random Forest Grid Search##
n_pred <- length(Boston)
rf_hyper_grid <- expand.grid(
mtry       = seq(5, n_pred, by = 2),  #number of predictors to use in each tree
n.trees    = c(300, 400, 500, 600),
node_size  = c(5),
sampe_size = c(.80),  #number of samples to use in each tree
OOB_RMSE   = 0
)
for(i in 1:nrow(rf_hyper_grid)) {
# train model
rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_hyper_grid$n.trees[i],
mtry            = rf_hyper_grid$mtry[i],
min.node.size   = rf_hyper_grid$node_size[i],
sample.fraction = rf_hyper_grid$sampe_size[i],
seed            = 123
)
# add OOB error to grid
rf_hyper_grid$OOB_RMSE[i] <- sqrt(rf_model$prediction.error)
}
rf_best_grid <- rf_hyper_grid %>%
dplyr::arrange(OOB_RMSE)
# train best model
best_rf_model <- ranger(
formula         = medv ~ .,
data            = Boston[train,],
num.trees       = rf_best_grid$n.trees[1],
mtry            = rf_best_grid$mtry[1],
min.node.size   = rf_best_grid$node_size[1],
sample.fraction = rf_best_grid$sampe_size[1],
seed            = 123
)
?svm
##SVM grid search code##
svm_fit <- function(kernel, equation, data){
svm_mod <- svm(equation, data = data, kernel = kernel, cross = 5) #fit svm model
return(svm_mod)
}
calc_mse <- function(model, test_data){
pred_columns <- attr(model$terms, "term.labels") #get list of data columns
test_x <- test_data[,pred_columns] #separate test x and y data
test_y <- as.vector(test_data[[toString(as.list(svm_model$terms)[[2]])]]) #retrieve y column from model and then retrieve y data
predicted_y <- as.vector(predict(model, newdata = test_x)) #predict new y values
mse <- mean((test_y - predicted_y) ^ 2) #calculate mse
return(mse)
}
svm_grid <- function(equation, data, test_data){
kernels <-  c("linear", "polynomial", "radial")
models <- lapply(kernels, svm_fit, equation = equation, data = data) #generate list of models
mse_list <- lapply(models, calc_mse, test_data = test_data) #calculate mse for each model
min_index <- which.min(mse_list) #find min mse and return it
return(models[[min_index]])
}
grid_svm_model <- svm_grid(medv~., data = Boston[train,], test_data = Boston[-train, ]) #returns grid search model
View(grid_svm_model)
svm_grid <- function(equation, data, test_data){
kernels <-  c("linear", "polynomial", "radial")
models <- lapply(kernels, svm_fit, equation = equation, data = data) #generate list of models
mse_list <- lapply(models, calc_mse, test_data = test_data) #calculate mse for each model
print(mse_list)
min_index <- which.min(mse_list) #find min mse and return it
return(models[[min_index]])
}
grid_svm_model <- svm_grid(medv~., data = Boston[train,], test_data = Boston[-train, ]) #returns grid search model
grid_svm_model$tot.MSE
?tune
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.1), cost = 2^(2:9))
)
View(tuneResult)
?svm
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = kernels)
)
kernels <-  c("linear", "polynomial", "radial")
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = kernels)
)
tuneResult
View(tuneResult)
?tune
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = kernels)
warnings()
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = kernels)
)
?svm
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "radial",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "linear",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "polynomial",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult
View(tuneResult)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "linear",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
View(tuneResult)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = c(0:2))
)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4), kernel = c(1:3))
)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = c(0:2),
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = 2,
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "2",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ], kernel = "radial",
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
View(tuneResult)
tuneResult <- tune(svm, medv ~ ., data = Boston[train, ],
ranges = list(epsilon = seq(0,1,0.2), cost = 2^(2:4))
)
