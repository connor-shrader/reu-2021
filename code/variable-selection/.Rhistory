ridge1_cv <- cv.glmnet(x = x_train, y = y_train,
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
ridge1_cv$lambda.min
coef(ridge1_cv, s = ridge1_cv$lambda.min)
coef(ridge1_cv, s = ridge1_cv$lambda.min)$Top25perc
coef(ridge1_cv, s = ridge1_cv$lambda.min)[2]
coef(ridge1_cv, s = ridge1_cv$lambda.min)[2]
as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
for (i in 1:100)
{
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
}
for (i in 1:100)
{
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
ridge.coefs[i,1] <- Beta1
ridge.coefs[i,2] <- Beta2
}
View(ridge.coefs)
install.packages("MASS")
install.packages("tidyverse")
for (i in 1:100)
{
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
ridge.coefs[i,1] <- Beta1
ridge.coefs[i,2] <- Beta2
lasso1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[3])
lasso.coefs[i,1] <- Beta1
lasso.coefs[i,2] <- Beta2
}
library(ISLR)
library(tidyverse)
library(MASS)
library(dplyr)
library(glmnet)
for (i in 1:100)
{
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
ridge.coefs[i,1] <- Beta1
ridge.coefs[i,2] <- Beta2
lasso1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[3])
lasso.coefs[i,1] <- Beta1
lasso.coefs[i,2] <- Beta2
}
View(ridge.coefs)
View(lasso.coefs)
sim <- function(rho){
#Number of samples to draw
N = 50
#Make a covariance matrix
covar = matrix(c(1,rho, rho, 1), byrow = T, nrow = 2)
#Append a column of 1s to N draws from a 2-dimensional gaussian
#With covariance matrix covar
X = cbind(rep(1,N),MASS::mvrnorm(N, mu = c(0,0), Sigma = covar))
#True betas for our regression
betas = c(1,2,4)
#Make the outcome
y = X%*%betas + rnorm(N,0,1)
#Fit a linear model
model = lm(y ~ X[,2] + X[,3])
#Return a dataframe of the coefficients
return(tibble(a1 = coef(model)[2], a2 = coef(model)[3]))
}
#Run the function 1000 times and stack the results
zero_covar = rerun(1000, sim(0)) %>%
bind_rows
#Same as above, but the covariance in covar matrix is now non-zero
high_covar = rerun(1000, sim(0.95)) %>% bind_rows
#plot
zero_covar %>%
ggplot(aes(a1,a2)) +
geom_point(data = high_covar, color = 'red') +
geom_point()
head(ridge1_cv)
sim <- function(rho){
#Number of samples to draw
N = 50
#Make a covariance matrix
covar = matrix(c(1,rho, rho, 1), byrow = T, nrow = 2)
#Append a column of 1s to N draws from a 2-dimensional gaussian
#With covariance matrix covar
X = cbind(rep(1,N),MASS::mvrnorm(N, mu = c(0,0), Sigma = covar))
#True betas for our regression
betas = c(1,2,4)
#Make the outcome
y = X%*%betas + rnorm(N,0,1)
#Fit a lasso model
model = cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#Return a dataframe of the coefficients
return(tibble(a1 = coef(model, model$lambda.min)[2], a2 = coef(model, model$lambda.min)[3]))
}
#Run the function 1000 times and stack the results
zero_covar = rerun(1000, sim(0)) %>%
bind_rows
#Same as above, but the covariance in covar matrix is now non-zero
high_covar = rerun(1000, sim(0.95)) %>% bind_rows
#plot
zero_covar %>%
ggplot(aes(a1,a2)) +
geom_point(data = high_covar, color = 'red') +
geom_point()
coef(model, model$lambda.min)
sim <- function(rho){
#Number of samples to draw
N = 50
#Make a covariance matrix
covar = matrix(c(1,rho, rho, 1), byrow = T, nrow = 2)
#Append a column of 1s to N draws from a 2-dimensional gaussian
#With covariance matrix covar
X = cbind(rep(1,N),MASS::mvrnorm(N, mu = c(0,0), Sigma = covar))
#True betas for our regression
betas = c(0,0.00062,0.441)
#Make the outcome
y = X%*%betas + rnorm(N,0,1)
#Fit a lasso model
model = cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#Return a dataframe of the coefficients
return(tibble(a1 = coef(model, model$lambda.min)[2], a2 = coef(model, model$lambda.min)[3]))
}
#Run the function 1000 times and stack the results
zero_covar = rerun(100, sim(0)) %>%
bind_rows
#Same as above, but the covariance in covar matrix is now non-zero
high_covar = rerun(100, sim(0.95)) %>% bind_rows
#plot
zero_covar %>%
ggplot(aes(a1,a2)) +
geom_point(data = high_covar, color = 'red') +
geom_point()
sim <- function(rho){
#Number of samples to draw
N = 50
#Make a covariance matrix
covar = matrix(c(1,rho, rho, 1), byrow = T, nrow = 2)
#Append a column of 1s to N draws from a 2-dimensional gaussian
#With covariance matrix covar
X = cbind(rep(1,N),MASS::mvrnorm(N, mu = c(0,0), Sigma = covar))
#True betas for our regression
betas = c(0,0.441,0.00062)
#Make the outcome
y = X%*%betas + rnorm(N,0,1)
#Fit a lasso model
model = cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#Return a dataframe of the coefficients
return(tibble(a1 = coef(model, model$lambda.min)[2], a2 = coef(model, model$lambda.min)[3]))
}
#Run the function 1000 times and stack the results
zero_covar = rerun(100, sim(0)) %>%
bind_rows
#Same as above, but the covariance in covar matrix is now non-zero
high_covar = rerun(100, sim(0.95)) %>% bind_rows
#plot
zero_covar %>%
ggplot(aes(a1,a2)) +
geom_point(data = high_covar, color = 'red') +
geom_point()
a1
aes
aes(a1)
head(aes(a1))
head(x)
sim <- function(rho){
#Number of samples to draw
N = 50
#Make a covariance matrix
covar = matrix(c(1,rho, rho, 1), byrow = T, nrow = 2)
#Append a column of 1s to N draws from a 2-dimensional gaussian
#With covariance matrix covar
X = cbind(rep(1,N),MASS::mvrnorm(N, mu = c(0,0), Sigma = covar))
#True betas for our regression
betas = c(0,0.441,0.00062)
#Make the outcome
y = X%*%betas + rnorm(N,0,1)
#Fit a lasso model
model = cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#Return a dataframe of the coefficients
return(tibble(a1 = coef(model, model$lambda.min)[2], a2 = coef(model, model$lambda.min)[3]))
}
#Run the function 1000 times and stack the results
zero_covar = rerun(100, sim(0)) %>%
bind_rows
#Same as above, but the covariance in covar matrix is now non-zero
high_covar = rerun(100, sim(0.95)) %>% bind_rows
#plot
zero_covar %>%
ggplot(aes(a1,a2)) +
geom_point(data = high_covar, color = 'red') +
geom_point()
tibble
head(tibble)
heaD(x)
head(x)
head(y)
aes(a2)
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
row_del <- sample(1:100, 1)
x_train <- x_full[-row_del, ]
y_train <- y_full[-row_del, ]
row_del <- sample(1:100, 1)
x_train <- x_full[, -row_del]
y_train <- y_full[, -row_del]
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
row_del <- sample(length(x_full), 1)
x_train <- x_full[, -row_del]
y_train <- y_full[, -row_del]
row_del <- sample(length(x_full), 1)
x_train <- x_full[, .-row_del]
y_train <- y_full[, .-row_del]
row_del <- sample(length(x_full), 1)
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del,]
row_del <- sample(1:length(x_full), 1)
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del,]
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
row_del <- sample(1:777, 1)
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del,]
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del,]
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del,]
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-c(row_del),]
y_train <- y_full[-c(row_del),]
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
library(ISLR)
library(tidyverse)
library(MASS)
library(dplyr)
library(glmnet)
#create empty dataframe of lasso and ridge coefs over 100 fits
lasso.coefs <- as.data.frame(matrix(0, ncol = 2, nrow = 100))
colnames(lasso.coefs) <- c("Beta1", "Beta2")
ridge.coefs <- as.data.frame(matrix(0, ncol = 2, nrow = 100))
colnames(ridge.coefs) <- c("Beta1", "Beta2")
#extract x training data from College database
x_full <- select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
View(x_train)
x_full <- dplyr::select(College, Top25perc, F.Undergrad)
y_full <- College$Grad.Rate
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-c(row_del),]
y_train <- y_full[-c(row_del),]
y_train <- y_full[-c(row_del)]
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-c(row_del),]
y_train <- y_full[-c(row_del)]
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del]
for (i in 1:100)
{
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del]
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
ridge.coefs[i,1] <- Beta1
ridge.coefs[i,2] <- Beta2
lasso1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[3])
lasso.coefs[i,1] <- Beta1
lasso.coefs[i,2] <- Beta2
}
View(ridge.coefs)
plot(lasso.coefs[,1], lasso.coefs[,2])
plot(ridge.coefs[,1], ridge.coefs[,1])
plot %>%
ggplot(aes(Beta1,Beta2)) +
geom_point(data = lasso.coefs, color = 'red') +
geom_point()
plot %>%
ggplot(aes(lasso.coefs,ridge.coefs)) +
geom_point(data = lasso.coefs, color = 'red') +
geom_point()
plot %>%
ggplot(aes(lasso.coefs,ridge.coefs)) +
geom_point(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, color = 'red')
plot %>%
ggplot(data = lasso.coefs, aes(Beta1, Beta2)) +
geom_point(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, color = 'red')
plot %>%
ggplot(data = lasso.coefs, aes(Beta1, Beta2)) +
geom_point(x = Beta1, y = Beta2, color = 'red')
plot %>%
ggplot(data = lasso.coefs, aes(Beta1, Beta2)) +
geom_point(color = 'red')
lasso.coefs %>%
ggplot(aes(Beta1, Beta2)) +
geom_point(color = 'red')
gplot(data = lasso.coefs, aes(Beta1, Beta2)) +
geom_point(color = 'red') +
ggplot(data = ridge.coefs, aes(Beta1,Beta2)) +
geom_point(color = "black")
ggplot(data = lasso.coefs, aes(Beta1, Beta2)) +
geom_point(color = 'red') +
ggplot(data = ridge.coefs, aes(Beta1,Beta2)) +
geom_point(color = "black")
plot(Beta1 ~ Beta2, data = lasso.coefs, col = "red",
Beta1 ~ Beta2, data = ridge.coefs, col = "black")
plot(lasso.coefs$Beta1 ~ lasso.coefs$Beta2, col = "red",
ridge.coefs$Beta1 ~ ridge.coefs$Beta2, col = "black")
plot(lasso.coefs$Beta1 ~ lasso.coefs$Beta2, col = "red",
ridge.coefs$Beta1 ~ ridge.coefs$Beta2, col = "black")
plot(lasso.coefs$Beta1, lasso.coefs$Beta2, col = "red",
ridge.coefs$Beta1, ridge.coefs$Beta2, col = "black")
plot(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, col = "red",
x = ridge.coefs$Beta1, y = ridge.coefs$Beta2, col = "black")
plot(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, col = "red")
points(ridge.coefs$Beta1, y = ridge.coefs$Beta2, col = "black")
plot(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, col = "red")
points(x = ridge.coefs$Beta1, y = ridge.coefs$Beta2, col = "black")
plot(x = ridge.coefs$Beta1, y = ridge.coefs$Beta2, col = "black")
#plot different coefficient values for each regression
plot(x = lasso.coefs$Beta1, y = lasso.coefs$Beta2, col = "red")
plot(x = ridge.coefs$Beta1, y = ridge.coefs$Beta2, col = "black")
#plot different coefficient values for each regression
plot(Beta1 ~ Beta2, data = lasso.coefs, col = "red")
plot(Beta1 ~ Beta2, data = ridge.coefs, col = "black")
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red")
plot(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red")
plot(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
head(College)
?College
#extract x training data from College database
x_full <- dplyr::select(College, Top25perc, Expend)
y_full <- College$Grad.Rate
for (i in 1:100)
{
#remove random row from training data
row_del <- as.numeric(sample(1:777, 1))
x_train <- x_full[-row_del,]
y_train <- y_full[-row_del]
ridge1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 0)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(ridge1_cv, s = ridge1_cv$lambda.min)[3])
ridge.coefs[i,1] <- Beta1
ridge.coefs[i,2] <- Beta2
lasso1_cv <- cv.glmnet(x = as.matrix(x_train), y = as.matrix(y_train),
## type.measure: loss to use for cross-validation.
type.measure = "mse",
## K = 10 is the default.
nfold = 10,
## ‘alpha = 1’ is the lasso penalty, and ‘alpha = 0’ the ridge penalty.
alpha = 1)
#record ridge regression coefficients
Beta1 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[2])
Beta2 <- as.numeric(coef(lasso1_cv, s = lasso1_cv$lambda.min)[3])
lasso.coefs[i,1] <- Beta1
lasso.coefs[i,2] <- Beta2
}
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red")
plot(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
coefs_tot <- rbind(lasso.coefs, ridge.coefs)
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend(legend=c("Lasso", "Ridge"), col = c("red", "black"))
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend(1, 95, legend=c("Lasso", "Ridge"), col = c("red", "black"))
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend(0.305, 0.00065, legend=c("Lasso", "Ridge"), col = c("red", "black"))
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend("topleft", legend=c("Lasso", "Ridge"), col = c("red", "black"))
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend("topleft", legend=c("Lasso", "Ridge"), col = c("red", "black"), cex=.8)
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend("topleft", legend=c("Lasso", "Ridge"), col = c("red", "black"), pch=c(1,2))
#plot different coefficient values for each regression
plot(Beta2 ~ Beta1, data = lasso.coefs, col = "red", xlim=c(min(coefs_tot$Beta1), max(coefs_tot$Beta1)), ylim=c(min(coefs_tot$Beta2), max(coefs_tot$Beta2)))
points(Beta2 ~ Beta1, data = ridge.coefs, col = "black")
legend("topleft", legend=c("Lasso", "Ridge"), col = c("red", "black"), pch=c(1,1))
