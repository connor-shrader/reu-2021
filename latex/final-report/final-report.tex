\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Penalized Regression in the Age of Big Data} %just a filler title, we can change it to whatever we want
\author{Gabriel Ackall$^1$*, Connor Shrader$^2$* \\
		Mentor: Dr. Ty Kim$^3$ \\	
		{\footnotesize $^1$Georgia Tech, Civil and Environmental Engineering} \\
		{\footnotesize $^2$University of Central  Florida, Mathematics} \\
		{\footnotesize $^3$NCA\&T University, Mathematics Department} \\
		{\footnotesize *Authors contributed equally}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
With the prevalence of big data in the modern age, the importance of modeling high dimensional data and selecting influential features has increased greatly. High dimensional data is common in many fields such as genome decoding, rare disease identification, economic modeling, and environmental modeling. However, most traditional regression and classification machine learning models are not designed to handle high dimensional data or conduct variable selection. In this paper, we investigated the use of penalized regression methods instead of, or in conjunction with, the traditional machine learning methods. We focused on lasso, ridge, elastic net, SCAD, MCP, and adaptive versions of lasso, ridge, and elastic net models. For traditional machine learning models, we focused on random forest models, gradient boosting models in the form of XGBoost, and support vector machines. These models were evaluated using factorial design methods for Monte Carlo simulations under various data environments. Tests were conducted for 270 environments, with factors being the number of predictors, number of samples, signal to noise ratio, covariance matrix, and correlation strength. This served to identify the strengths and weaknesses of different penalization techniques in different environments. We also compared different models using empirical datasets to test their viability in real-world scenarios. Additionally, we considered penalization methods outlined earlier in logistic regression models for classifying data. These results were compared to random forest, gradient boosting, and support vector machine classification models using both Monte Carlo data generation methods and empirical data. For regression, we evaluated the models using the test mean squared error and variable selection accuracy; for classification, we considered test prediction accuracy and variable selection accuracy. We found that for both regression and classification, penalized regression models outperformed more traditional machine learning algorithms in most high-dimensional situations or in situations with a low number of data observations. By comparing traditional machine learning methods with penalized regression, we hope to expand the scope of machine learning methods for big data to include the various penalized regression techniques we tested. Additionally, we hope to create a greater understanding of the strengths and weaknesses of each model type and provide a reference for other researchers on which machine learning techniques they should use, depending on a range of factors. \\

\textit{Keywords:} penalized regression, variable selection, classification, machine learning, large p little n problem, Monte Carlo simulations
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
% introduce readers to our topic and necessary info to better understand the paper
\subsection{Background}
% We can talk about the necessary background information
% what is linear regression and logistic regression?

\subsection{Literature Review}
% We can review some relevant literature such as Tibshrani et. al. lasso methods, ridge regression, enet, SCAD, or whatever else we want to do.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
\subsection{Models}
% lists and describes every tested model. Can include equations for models, but should not go into such great depth about each model: that can be reserved for literature review portion of the paper.

\subsection{Monte Carlo Simulations}
% outline data generation process and factorial design process

\subsection{Empirical Data}
% outline empirical data sources and what each database is about / what we try to predict about it

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\subsection{Regression Models}
% Stick all the results from the linear regression models here

\subsection{Classification Models}
% stick the results from logisitc regression and classification models here

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
% discussion of findings, future work, and contributions
\subsection{Findings}
% overall ideas that we found out after all these results. Which type of model is best in which environment, etc?
% Might need to break it into sub sub sections for classification vs regression or monte carlo vs empirical???
\subsection{Contributions}
% what are our contributions, why is what we did important?
% we have a comprehensive analysis of 21 models in 270 different environments. Allows for comprehensive comparison and gives guidance to data scientists as to which model they should use for their research
% empirical data methods show the important uses of penalized regression in environmental, biological, and etc. data
\subsection{Future Work}
% maybe talk about the penalty method we came up with? Talk about how it might be able to solve issues of Logsum method?

\end{document}