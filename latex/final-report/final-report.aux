\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Modeling Background}{2}{subsection.1.1}\protected@file@percent }
\newlabel{eqn:relationship}{{1}{2}{Modeling Background}{equation.1.1}{}}
\citation{friedman2001elements}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear Regression and Ordinary Least Squares}{3}{subsection.1.2}\protected@file@percent }
\newlabel{eqn:linear-model}{{2}{3}{Linear Regression and Ordinary Least Squares}{equation.1.2}{}}
\newlabel{eqn:RSS}{{4}{3}{Linear Regression and Ordinary Least Squares}{equation.1.4}{}}
\newlabel{eqn:ols-solution}{{6}{3}{Linear Regression and Ordinary Least Squares}{equation.1.6}{}}
\citation{liu2020logsum}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ols}{{1}{4}{Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Subset Selection Methods}{5}{subsection.1.3}\protected@file@percent }
\citation{james2017islr}
\citation{james2017islr}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Penalized Regression}{6}{subsection.1.4}\protected@file@percent }
\citation{james2017islr}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2017islr}
\citation{james2013introduction}
\newlabel{eqn:penalized-regression-lambda}{{9}{7}{Penalized Regression}{equation.1.9}{}}
\newlabel{eqn:penalized-regression-t}{{10}{7}{Penalized Regression}{equation.1.10}{}}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\newlabel{fig:ridge-diagram}{{2a}{8}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{8}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{fig:lasso-diagram}{{2b}{8}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{8}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by lasso and ridge.\relax }}{8}{figure.caption.2}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{2}{8}{RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by lasso and ridge.\relax }{figure.caption.2}{}}
\citation{fan2001variable}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\citation{breheny2016lasso}
\newlabel{eqn:scad-derivative-indicator}{{12}{9}{Penalized Regression}{equation.1.12}{}}
\citation{fan2001variable}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{fig:penalty}{{3a}{10}{Penalty functions for lasso, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:penalty}{{a}{10}{Penalty functions for lasso, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{fig:derivative}{{3b}{10}{Derivatives of the penalty functions for lasso, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:derivative}{{b}{10}{Derivatives of the penalty functions for lasso, SCAD, and MCP\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{3}{10}{Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.3}{}}
\newlabel{ols-orthonormal-solution}{{17}{10}{Penalized Regression}{equation.1.17}{}}
\citation{donoho1994ideal}
\citation{fan2001variable}
\citation{zhang2010nearly}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Thresholding function for LASSO, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input (z) represents an entry from the vector $\mathbf  {z}=\mathbf  {X}^\top \mathbf  {y}$; the output ($\hat  {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:prediction}{{4}{11}{Thresholding function for LASSO, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input (z) represents an entry from the vector $\mathbf {z}=\mathbf {X}^\top \mathbf {y}$; the output ($\hat {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Non-linear models}{11}{subsection.1.5}\protected@file@percent }
\citation{james2013introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of a decision tree. This tree was generated using the \lstinline !iris! data set and attempts to predict the species of a flower based on its petal length and petal width. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:decision-tree}{{5}{12}{Example of a decision tree. This tree was generated using the \lstinline !iris! data set and attempts to predict the species of a flower based on its petal length and petal width. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{14}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Models}{14}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \lstinline !R! Libraries used and the models used from each library\relax }}{14}{table.caption.6}\protected@file@percent }
\newlabel{tab:model-libraries}{{1}{14}{\lstinline !R! Libraries used and the models used from each library\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Monte Carlo Simulations}{16}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:symmetric-compound-matrix}{{21}{17}{Monte Carlo Simulations}{equation.2.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Empirical Data}{18}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of BRCA1 gene expression levels. The mean gene expression is -1.459.\relax }}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:BRCA1-distribution}{{6}{18}{Distribution of BRCA1 gene expression levels. The mean gene expression is -1.459.\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{19}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Monte Carlo Simulations}{19}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:train-mse-1000-10}{{7}{21}{Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Average mean squared error using the test data for all simulated models when $n = 1000$ and $p = 10$.\relax }}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:test-mse-1000-10}{{8}{21}{Average mean squared error using the test data for all simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average number of non-zero coefficients that were correctly estimated as non-zero when $n = 1000$ and $p = 10$.\relax }}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:tp-1000-10}{{9}{22}{Average number of non-zero coefficients that were correctly estimated as non-zero when $n = 1000$ and $p = 10$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average number of zero coefficients that were incorrectly estimated as non-zero when $n = 1000$ and $p = 10$.\relax }}{22}{figure.caption.10}\protected@file@percent }
\newlabel{fig:fp-1000-10}{{10}{22}{Average number of zero coefficients that were incorrectly estimated as non-zero when $n = 1000$ and $p = 10$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average mean square error using training data for Monte Carlo models when $n = 50$ and $p = 2000$.\relax }}{23}{figure.caption.11}\protected@file@percent }
\newlabel{fig:train-mse-50-2000}{{11}{23}{Average mean square error using training data for Monte Carlo models when $n = 50$ and $p = 2000$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Average mean square error using testing data for Monte Carlo models when $n = 50$ and $p = 2000$.\relax }}{23}{figure.caption.11}\protected@file@percent }
\newlabel{fig:test-mse-50-2000}{{12}{23}{Average mean square error using testing data for Monte Carlo models when $n = 50$ and $p = 2000$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average number of non-zero coefficients that were correctly estimated as non-zero when $n = 50$ and $p = 2000$.\relax }}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:tp-50-2000}{{13}{24}{Average number of non-zero coefficients that were correctly estimated as non-zero when $n = 50$ and $p = 2000$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Average number of zero coefficients that were incorrectly estimated as non-zero when $n = 50$ and $p = 2000$.\relax }}{25}{figure.caption.13}\protected@file@percent }
\newlabel{fig:fp-50-2000}{{14}{25}{Average number of zero coefficients that were incorrectly estimated as non-zero when $n = 50$ and $p = 2000$.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Data}{25}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }}{25}{table.caption.15}\protected@file@percent }
\newlabel{tab:emp_results}{{2}{25}{Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }{table.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{fig:empirical_mse}{{15}{26}{Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }{figure.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Average Runtimes for Empirical Data Models\relax }}{26}{table.caption.17}\protected@file@percent }
\newlabel{tab:emp_runtimes}{{3}{26}{Average Runtimes for Empirical Data Models\relax }{table.caption.17}{}}
\newlabel{fig:venn1}{{16a}{27}{First fold\relax }{figure.caption.16}{}}
\newlabel{sub@fig:venn1}{{a}{27}{First fold\relax }{figure.caption.16}{}}
\newlabel{fig:venn2}{{16b}{27}{Second fold\relax }{figure.caption.16}{}}
\newlabel{sub@fig:venn2}{{b}{27}{Second fold\relax }{figure.caption.16}{}}
\newlabel{fig:venn3}{{16c}{27}{Third fold\relax }{figure.caption.16}{}}
\newlabel{sub@fig:venn3}{{c}{27}{Third fold\relax }{figure.caption.16}{}}
\newlabel{fig:venn4}{{16d}{27}{Fourth Fold\relax }{figure.caption.16}{}}
\newlabel{sub@fig:venn4}{{d}{27}{Fourth Fold\relax }{figure.caption.16}{}}
\newlabel{fig:venn5}{{16e}{27}{Fifth Fold\relax }{figure.caption.16}{}}
\newlabel{sub@fig:venn5}{{e}{27}{Fifth Fold\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Venn diagrams with number of common predictors selected by each regression model.\relax }}{27}{figure.caption.16}\protected@file@percent }
\newlabel{fig:venn}{{16}{27}{Venn diagrams with number of common predictors selected by each regression model.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{27}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Findings}{27}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Monte Carlo Simulations}{27}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Empirical Data}{29}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Contributions}{30}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Work}{30}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Full Results}{31}{appendix.A}\protected@file@percent }
\newlabel{app:full-results}{{A}{31}{Full Results}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }}{32}{table.4}\protected@file@percent }
\newlabel{tab:train-mse-1000-10}{{4}{32}{Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Test2\relax }}{33}{table.5}\protected@file@percent }
\newlabel{tab:test-mse-1000-10}{{5}{33}{Test2\relax }{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Test3\relax }}{34}{table.6}\protected@file@percent }
\newlabel{tab:train-mse-50-2000}{{6}{34}{Test3\relax }{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Test4\relax }}{34}{table.7}\protected@file@percent }
\newlabel{tab:test-mse-50-2000}{{7}{34}{Test4\relax }{table.7}{}}
\bibstyle{plain}
\bibdata{references}
\bibcite{breheny2016lasso}{1}
\bibcite{donoho1994ideal}{2}
\bibcite{fan2001variable}{3}
\bibcite{friedman2001elements}{4}
\bibcite{hoerl1970ridge}{5}
\bibcite{james2013introduction}{6}
\bibcite{james2017islr}{7}
\bibcite{liu2020logsum}{8}
\bibcite{tibshirani1996regression}{9}
\bibcite{zhang2010nearly}{10}
\bibcite{zou2005regularization}{11}
\gdef \@abspage@last{35}
