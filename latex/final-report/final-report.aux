\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\citation{hoerl1970ridge}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Modeling Background}{4}{subsection.2.1}\protected@file@percent }
\newlabel{eqn:relationship}{{1}{4}{Modeling Background}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Regression and Ordinary Least Squares}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:linear-model}{{2}{4}{Linear Regression and Ordinary Least Squares}{equation.2.2}{}}
\newlabel{eqn:RSS}{{4}{5}{Linear Regression and Ordinary Least Squares}{equation.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ols}{{1}{5}{Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }{figure.caption.2}{}}
\citation{friedman2001elements}
\citation{liu2020logsum}
\citation{sanchez2007filter}
\citation{ding2005minimum}
\newlabel{eqn:ols-solution}{{6}{6}{Linear Regression and Ordinary Least Squares}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Subset Selection Methods}{7}{subsection.2.3}\protected@file@percent }
\citation{akaike1998information}
\citation{schwarz1978estimating}
\citation{james2013introduction}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Penalized Regression}{8}{subsection.2.4}\protected@file@percent }
\citation{james2013introduction}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2013introduction}
\newlabel{eqn:penalized-regression-lambda}{{10}{9}{Penalized Regression}{equation.2.10}{}}
\newlabel{eqn:penalized-regression-t}{{11}{9}{Penalized Regression}{equation.2.11}{}}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\newlabel{fig:ridge-diagram}{{2a}{10}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{10}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{fig:lasso-diagram}{{2b}{10}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{10}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by lasso and ridge.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{2}{10}{RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by lasso and ridge.\relax }{figure.caption.3}{}}
\citation{fan2001variable}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\newlabel{eqn:scad-derivative-indicator}{{13}{11}{Penalized Regression}{equation.2.13}{}}
\citation{fan2001variable}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{fig:penalty}{{3a}{12}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:penalty}{{a}{12}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{fig:derivative}{{3b}{12}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:derivative}{{b}{12}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{3}{12}{Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.4}{}}
\newlabel{ols-orthonormal-solution}{{18}{12}{Penalized Regression}{equation.2.18}{}}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{donoho1994ideal}
\citation{zou2006adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input (z) represents an entry from the vector $\mathbf  {z}=\mathbf  {X}^\top \mathbf  {y}$; the output ($\hat  {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{13}{figure.caption.5}\protected@file@percent }
\newlabel{fig:prediction}{{4}{13}{Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input (z) represents an entry from the vector $\mathbf {z}=\mathbf {X}^\top \mathbf {y}$; the output ($\hat {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.5}{}}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{james2013introduction}
\citation{breiman2001random}
\citation{efron1994introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Non-linear models}{14}{subsection.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of a decision tree. This tree was generated using the \lstinline !iris! data set and attempts to predict the species of a flower based on its petal length and petal width. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:decision-tree}{{5}{14}{Example of a decision tree. This tree was generated using the \lstinline !iris! data set and attempts to predict the species of a flower based on its petal length and petal width. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }{figure.caption.6}{}}
\citation{breiman1996bagging}
\citation{schapire1990strength}
\citation{friedman2001greedy}
\citation{chen2016xgboost}
\citation{cortes1995support}
\citation{drucker1997support}
\citation{r}
\citation{venables2002mass}
\citation{friedman2010regularization}
\citation{breheny2011ncvreg}
\citation{chen2021xgboost}
\citation{wright2017ranger}
\citation{liaw2002rf}
\citation{meyer2021e1071}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{16}{section.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \lstinline !R! Libraries used and the models used from each library\relax }}{17}{table.caption.7}\protected@file@percent }
\newlabel{tab:model-libraries}{{1}{17}{\lstinline !R! Libraries used and the models used from each library\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Monte Carlo Simulations}{18}{section.4}\protected@file@percent }
\newlabel{eqn:symmetric-compound-matrix}{{23}{19}{Monte Carlo Simulations}{equation.4.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Empirical Data}{20}{section.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Distribution of BRCA1 gene expression levels. This is the variable that we used as the response. The mean gene expression is -1.459.\relax }}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:BRCA1-distribution}{{6}{21}{Distribution of BRCA1 gene expression levels. This is the variable that we used as the response. The mean gene expression is -1.459.\relax }{figure.caption.8}{}}
\citation{liu2020logsum}
\@writefile{toc}{\contentsline {section}{\numberline {6}Monte Carlo Simulation Results}{22}{section.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }}{24}{figure.caption.9}\protected@file@percent }
\newlabel{fig:train-mse-1000-10}{{7}{24}{Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Average mean squared error using the test data for simulated models when $n = 1000$ and $p = 10$.\relax }}{24}{figure.caption.9}\protected@file@percent }
\newlabel{fig:test-mse-1000-10}{{8}{24}{Average mean squared error using the test data for simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average $\beta $-sensitivity for simulated models when $n = 1000$ and $p = 10$.\relax }}{25}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sensitivity-1000-10}{{9}{25}{Average $\beta $-sensitivity for simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average $\beta $-specificity for simulated models when $n = 1000$ and $p = 10$.\relax }}{25}{figure.caption.11}\protected@file@percent }
\newlabel{fig:specificity-1000-10}{{10}{25}{Average $\beta $-specificity for simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Empirical Data Results}{26}{section.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }}{26}{table.caption.16}\protected@file@percent }
\newlabel{tab:emp_results}{{2}{26}{Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }{table.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average mean square error using training data for simulated models when $n = 50$ and $p = 2000$.\relax }}{27}{figure.caption.12}\protected@file@percent }
\newlabel{fig:train-mse-50-2000}{{11}{27}{Average mean square error using training data for simulated models when $n = 50$ and $p = 2000$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Average mean square error using testing data for simulated models when $n = 50$ and $p = 2000$.\relax }}{27}{figure.caption.12}\protected@file@percent }
\newlabel{fig:test-mse-50-2000}{{12}{27}{Average mean square error using testing data for simulated models when $n = 50$ and $p = 2000$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average $\beta $-sensitivity for simulated models when $n = 50$ and $p = 2000$\relax }}{28}{figure.caption.13}\protected@file@percent }
\newlabel{fig:sensitivity-50-2000}{{13}{28}{Average $\beta $-sensitivity for simulated models when $n = 50$ and $p = 2000$\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Average $\beta $-specificity for simulated models when $n = 50$ and $p = 2000$.\relax }}{28}{figure.caption.14}\protected@file@percent }
\newlabel{fig:specificity-50-2000}{{14}{28}{Average $\beta $-specificity for simulated models when $n = 50$ and $p = 2000$.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }}{29}{figure.caption.15}\protected@file@percent }
\newlabel{fig:empirical_mse}{{15}{29}{Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Average Runtimes for Empirical Data Models\relax }}{29}{table.caption.18}\protected@file@percent }
\newlabel{tab:emp_runtimes}{{3}{29}{Average Runtimes for Empirical Data Models\relax }{table.caption.18}{}}
\newlabel{fig:venn1}{{16a}{30}{First fold\relax }{figure.caption.17}{}}
\newlabel{sub@fig:venn1}{{a}{30}{First fold\relax }{figure.caption.17}{}}
\newlabel{fig:venn2}{{16b}{30}{Second fold\relax }{figure.caption.17}{}}
\newlabel{sub@fig:venn2}{{b}{30}{Second fold\relax }{figure.caption.17}{}}
\newlabel{fig:venn3}{{16c}{30}{Third fold\relax }{figure.caption.17}{}}
\newlabel{sub@fig:venn3}{{c}{30}{Third fold\relax }{figure.caption.17}{}}
\newlabel{fig:venn4}{{16d}{30}{Fourth Fold\relax }{figure.caption.17}{}}
\newlabel{sub@fig:venn4}{{d}{30}{Fourth Fold\relax }{figure.caption.17}{}}
\newlabel{fig:venn5}{{16e}{30}{Fifth Fold\relax }{figure.caption.17}{}}
\newlabel{sub@fig:venn5}{{e}{30}{Fifth Fold\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Venn diagrams of the predictors selected by lasso, elastic-net, MCP, and random forest models for each of the five folds. Each number represents the number of important predictors chosen by the models that overlap the number.\relax }}{30}{figure.caption.17}\protected@file@percent }
\newlabel{fig:venn}{{16}{30}{Venn diagrams of the predictors selected by lasso, elastic-net, MCP, and random forest models for each of the five folds. Each number represents the number of important predictors chosen by the models that overlap the number.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion of Monte Carlo Results}{30}{section.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9}Discussion of Empirical Data Results}{32}{section.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10}Contributions}{32}{section.10}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11}Future Work}{32}{section.11}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Full Results}{33}{appendix.A}\protected@file@percent }
\newlabel{app:full-results}{{A}{33}{Full Results}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }}{34}{table.4}\protected@file@percent }
\newlabel{tab:train-mse-1000-10}{{4}{34}{Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Test2\relax }}{35}{table.5}\protected@file@percent }
\newlabel{tab:test-mse-1000-10}{{5}{35}{Test2\relax }{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Test3\relax }}{36}{table.6}\protected@file@percent }
\newlabel{tab:train-mse-50-2000}{{6}{36}{Test3\relax }{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Test4\relax }}{36}{table.7}\protected@file@percent }
\newlabel{tab:test-mse-50-2000}{{7}{36}{Test4\relax }{table.7}{}}
\bibstyle{plain-lf}
\bibdata{references}
\bibcite{akaike1998information}{1}
\bibcite{breheny2016lasso}{2}
\bibcite{breheny2011ncvreg}{3}
\bibcite{breiman1996bagging}{4}
\bibcite{breiman2001random}{5}
\bibcite{chen2016xgboost}{6}
\bibcite{chen2021xgboost}{7}
\bibcite{cortes1995support}{8}
\bibcite{ding2005minimum}{9}
\bibcite{donoho1994ideal}{10}
\bibcite{drucker1997support}{11}
\bibcite{efron1994introduction}{12}
\bibcite{fan2001variable}{13}
\bibcite{friedman2010regularization}{14}
\bibcite{friedman2001elements}{15}
\bibcite{friedman2001greedy}{16}
\bibcite{hoerl1970ridge}{17}
\bibcite{james2013introduction}{18}
\bibcite{liaw2002rf}{19}
\bibcite{liu2020logsum}{20}
\bibcite{meyer2021e1071}{21}
\bibcite{r}{22}
\bibcite{sanchez2007filter}{23}
\bibcite{schapire1990strength}{24}
\bibcite{schwarz1978estimating}{25}
\bibcite{tibshirani1996regression}{26}
\bibcite{venables2002mass}{27}
\bibcite{wright2017ranger}{28}
\bibcite{zhang2010nearly}{29}
\bibcite{zou2006adaptive}{30}
\bibcite{zou2005regularization}{31}
\gdef \@abspage@last{38}
