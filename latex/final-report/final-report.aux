\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Modeling Background}{2}{subsection.1.1}\protected@file@percent }
\newlabel{eqn:relationship}{{1}{2}{Modeling Background}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear Regression and Ordinary Least Squares}{2}{subsection.1.2}\protected@file@percent }
\newlabel{eqn:linear-model}{{2}{2}{Linear Regression and Ordinary Least Squares}{equation.1.2}{}}
\citation{friedman2001elements}
\newlabel{eqn:RSS}{{4}{3}{Linear Regression and Ordinary Least Squares}{equation.1.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ols}{{1}{3}{Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }{figure.caption.1}{}}
\newlabel{eqn:ols-solution}{{6}{3}{Linear Regression and Ordinary Least Squares}{equation.1.6}{}}
\citation{liu2020logsum}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Subset Selection Methods}{4}{subsection.1.3}\protected@file@percent }
\citation{james2017islr}
\citation{james2017islr}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Penalized Regression}{5}{subsection.1.4}\protected@file@percent }
\citation{james2017islr}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2017islr}
\citation{james2013introduction}
\newlabel{eqn:penalized-regression-lambda}{{9}{6}{Penalized Regression}{equation.1.9}{}}
\newlabel{eqn:penalized-regression-t}{{10}{6}{Penalized Regression}{equation.1.10}{}}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{fan2001variable}
\newlabel{fig:ridge-diagram}{{2a}{7}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{7}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{fig:lasso-diagram}{{2b}{7}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{7}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RSS contours and penalty bounds for the ridge and LASSO models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and LASSO regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by LASSO and ridge.\relax }}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{2}{7}{RSS contours and penalty bounds for the ridge and LASSO models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and LASSO regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by LASSO and ridge.\relax }{figure.caption.2}{}}
\newlabel{eqn:scad-derivative-indicator}{{12}{7}{Penalized Regression}{equation.1.12}{}}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\citation{breheny2016lasso}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{fig:penalty}{{3a}{8}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:penalty}{{a}{8}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{fig:derivative}{{3b}{8}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:derivative}{{b}{8}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{3}{8}{Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.3}{}}
\citation{donoho1994ideal}
\citation{fan2001variable}
\citation{zhang2010nearly}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Thresholding function for LASSO, SCAD, and MCP when $\lambda = 2$ and $a = 3$. In an orthonormal design, this function can be applied to the true coefficient values to get the predicted coefficient values. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:prediction}{{4}{9}{Thresholding function for LASSO, SCAD, and MCP when $\lambda = 2$ and $a = 3$. In an orthonormal design, this function can be applied to the true coefficient values to get the predicted coefficient values. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Non-linear models}{9}{subsection.1.5}\protected@file@percent }
\citation{james2013introduction}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{11}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Models}{11}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \lstinline !R! Libraries used and the models used from each library\relax }}{11}{table.caption.5}\protected@file@percent }
\newlabel{tab:model-libraries}{{1}{11}{\lstinline !R! Libraries used and the models used from each library\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Monte Carlo Simulations}{12}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:symmetric-compound-matrix}{{20}{13}{Monte Carlo Simulations}{equation.2.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Empirical Data}{14}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of BRCA1 gene expression levels\relax }}{15}{figure.caption.6}\protected@file@percent }
\newlabel{fig:BRCA1-distribution}{{5}{15}{Distribution of BRCA1 gene expression levels\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{15}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Monte Carlo Simulations}{15}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Data}{15}{subsection.3.2}\protected@file@percent }
\newlabel{fig:empirical_mse}{{\caption@xref {fig:empirical_mse}{ on input line 494}}{16}{Empirical Data}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average Mean Square Error for Empirical Data Models\relax }}{16}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{16}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Findings}{16}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Contributions}{16}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Work}{16}{subsection.4.3}\protected@file@percent }
\bibstyle{plain}
\bibdata{references}
\bibcite{breheny2016lasso}{1}
\bibcite{donoho1994ideal}{2}
\bibcite{fan2001variable}{3}
\bibcite{friedman2001elements}{4}
\bibcite{hoerl1970ridge}{5}
\bibcite{james2013introduction}{6}
\bibcite{james2017islr}{7}
\bibcite{liu2020logsum}{8}
\bibcite{tibshirani1996regression}{9}
\bibcite{zhang2010nearly}{10}
\bibcite{zou2005regularization}{11}
\gdef \@abspage@last{18}
