\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Modeling Background}{2}{subsection.1.1}\protected@file@percent }
\newlabel{eqn:relationship}{{1}{2}{Modeling Background}{equation.1.1}{}}
\citation{friedman2001elements}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear Regression and Ordinary Least Squares}{3}{subsection.1.2}\protected@file@percent }
\newlabel{eqn:linear-model}{{2}{3}{Linear Regression and Ordinary Least Squares}{equation.1.2}{}}
\newlabel{eqn:RSS}{{4}{3}{Linear Regression and Ordinary Least Squares}{equation.1.4}{}}
\newlabel{eqn:ols-solution}{{6}{3}{Linear Regression and Ordinary Least Squares}{equation.1.6}{}}
\citation{liu2020logsum}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ols}{{1}{4}{Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Subset Selection Methods}{5}{subsection.1.3}\protected@file@percent }
\citation{james2017islr}
\citation{james2017islr}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Penalized Regression}{6}{subsection.1.4}\protected@file@percent }
\citation{james2017islr}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2017islr}
\citation{james2013introduction}
\newlabel{eqn:penalized-regression-lambda}{{9}{7}{Penalized Regression}{equation.1.9}{}}
\newlabel{eqn:penalized-regression-t}{{10}{7}{Penalized Regression}{equation.1.10}{}}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\newlabel{fig:ridge-diagram}{{2a}{8}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{8}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{fig:lasso-diagram}{{2b}{8}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{8}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RSS contours and penalty bounds for the ridge and LASSO models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and LASSO regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by LASSO and ridge.\relax }}{8}{figure.caption.2}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{2}{8}{RSS contours and penalty bounds for the ridge and LASSO models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and LASSO regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region represents the point selected by LASSO and ridge.\relax }{figure.caption.2}{}}
\citation{fan2001variable}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\citation{breheny2016lasso}
\newlabel{eqn:scad-derivative-indicator}{{12}{9}{Penalized Regression}{equation.1.12}{}}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{fig:penalty}{{3a}{10}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:penalty}{{a}{10}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{fig:derivative}{{3b}{10}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\newlabel{sub@fig:derivative}{{b}{10}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{3}{10}{Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.3}{}}
\citation{donoho1994ideal}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{james2013introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Thresholding function for LASSO, SCAD, and MCP when $\lambda = 2$ and $a = 3$. In an orthonormal design, this function can be applied to the true coefficient values to get the predicted coefficient values. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:prediction}{{4}{11}{Thresholding function for LASSO, SCAD, and MCP when $\lambda = 2$ and $a = 3$. In an orthonormal design, this function can be applied to the true coefficient values to get the predicted coefficient values. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Non-linear models}{11}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{13}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Models}{13}{subsection.2.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \lstinline !R! Libraries used and the models used from each library\relax }}{14}{table.caption.5}\protected@file@percent }
\newlabel{tab:model-libraries}{{1}{14}{\lstinline !R! Libraries used and the models used from each library\relax }{table.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Monte Carlo Simulations}{15}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:symmetric-compound-matrix}{{20}{16}{Monte Carlo Simulations}{equation.2.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Empirical Data}{17}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribution of BRCA1 gene expression levels. The mean gene expression is -1.459.\relax }}{18}{figure.caption.6}\protected@file@percent }
\newlabel{fig:BRCA1-distribution}{{5}{18}{Distribution of BRCA1 gene expression levels. The mean gene expression is -1.459.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{18}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Monte Carlo Simulations}{18}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Empirical Data}{19}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }}{20}{figure.caption.7}\protected@file@percent }
\newlabel{fig:train-mse-1000-10}{{6}{20}{Average mean squared error using the training data for all simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Average mean squared error using the test data for all simulated models when $n = 1000$ and $p = 10$.\relax }}{20}{figure.caption.7}\protected@file@percent }
\newlabel{fig:test-mse-1000-10}{{7}{20}{Average mean squared error using the test data for all simulated models when $n = 1000$ and $p = 10$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Average Mean Square Error for Empirical Data Models\relax }}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:train-mse-50-2000}{{8}{21}{Average Mean Square Error for Empirical Data Models\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average Mean Square Error for Empirical Data Models\relax }}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:test-mse-50-2000}{{9}{21}{Average Mean Square Error for Empirical Data Models\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{21}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Findings}{21}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Contributions}{21}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Future Work}{21}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average Mean Square Error for Empirical Data Models\relax }}{22}{figure.caption.9}\protected@file@percent }
\newlabel{fig:empirical_mse}{{10}{22}{Average Mean Square Error for Empirical Data Models\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Full Results}{22}{appendix.A}\protected@file@percent }
\newlabel{app:full-results}{{A}{22}{Full Results}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }}{23}{table.2}\protected@file@percent }
\newlabel{tab:train-mse-1000-10}{{2}{23}{Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$.\relax }{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Test2\relax }}{24}{table.3}\protected@file@percent }
\newlabel{tab:test-mse-1000-10}{{3}{24}{Test2\relax }{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Test3\relax }}{25}{table.4}\protected@file@percent }
\newlabel{tab:train-mse-50-2000}{{4}{25}{Test3\relax }{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Test4\relax }}{25}{table.5}\protected@file@percent }
\newlabel{tab:test-mse-50-2000}{{5}{25}{Test4\relax }{table.5}{}}
\bibstyle{plain}
\bibdata{references}
\bibcite{breheny2016lasso}{1}
\bibcite{donoho1994ideal}{2}
\bibcite{fan2001variable}{3}
\bibcite{friedman2001elements}{4}
\bibcite{hoerl1970ridge}{5}
\bibcite{james2013introduction}{6}
\bibcite{james2017islr}{7}
\bibcite{liu2020logsum}{8}
\bibcite{tibshirani1996regression}{9}
\bibcite{zhang2010nearly}{10}
\bibcite{zou2005regularization}{11}
\gdef \@abspage@last{26}
