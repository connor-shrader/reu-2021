\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{eqn:linear-model}{{1}{2}{Introduction}{equation.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Ordinary Least Squares}{2}{subsection.1.1}\protected@file@percent }
\citation{buehlmann2006boosting}
\citation{genuer2008random}
\citation{capitaine2021random}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Linear Regression with High Dimensionality Data}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Alternative Techniques with High Dimensionality Data}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Subset Selection}{3}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Best Subset Selection}{3}{subsection.2.1}\protected@file@percent }
\citation{james2013introduction}
\citation{lumley2020leaps}
\citation{james2017islr}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Number of fitted models depending on number of predictors (p)\relax }}{4}{table.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:subset-combinations}{{1}{4}{Number of fitted models depending on number of predictors (p)\relax }{table.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $R^2$ and BIC when applying best subset selection\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:best-subset-selection}{{1}{4}{$R^2$ and BIC when applying best subset selection\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Forward Stepwise Selection}{4}{subsection.2.2}\protected@file@percent }
\citation{friedman2001elements}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $R^2$ and BIC when applying forward stepwise selection\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:forward-stepwise-selection}{{2}{5}{$R^2$ and BIC when applying forward stepwise selection\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Backward Stepwise Selection}{5}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $R^2$ and BIC when applying backward stepwise selection\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:backward-stepwise-selection}{{3}{6}{$R^2$ and BIC when applying backward stepwise selection\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Hybrid Stepwise Selection}{6}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Forward Stagewise Selection}{6}{subsection.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Penalized Regression}{6}{section.3}\protected@file@percent }
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2017islr}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Ridge Regression}{7}{subsection.3.1}\protected@file@percent }
\newlabel{ridge_reg}{{8}{7}{Ridge Regression}{equation.3.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Lasso Regression}{8}{subsection.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error and constant curves for the lasso and ridge models when $p=2$.\relax }}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ridge-lasso}{{4}{8}{Error and constant curves for the lasso and ridge models when $p=2$.\relax }{figure.caption.6}{}}
\citation{zou2005regularization}
\citation{zou2006adaptive}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Instability of Lasso and Ridge Regression to Changes in Training Data\relax }}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:lasso-and-ridge-instability}{{5}{9}{Instability of Lasso and Ridge Regression to Changes in Training Data\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Elastic Net Regression}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Adaptive Lasso Regression}{9}{subsection.3.4}\protected@file@percent }
\citation{fan2001variable}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\citation{breheny2016lasso}
\newlabel{adap_lasso}{{14}{10}{Adaptive Lasso Regression}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Smoothly Clipped Absolute Deviation Regression}{10}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Minimax Concave Penalty Regression}{10}{subsection.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$.\relax }}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{6}{11}{Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$.\relax }{figure.caption.8}{}}
\newlabel{fig:penalty}{{6a}{11}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.8}{}}
\newlabel{sub@fig:penalty}{{a}{11}{Penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.8}{}}
\newlabel{fig:derivative}{{6b}{11}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.8}{}}
\newlabel{sub@fig:derivative}{{b}{11}{Derivatives of the penalty functions for LASSO, SCAD, and MCP\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Solutions for LASSO, SCAD, and MCP for a single predictor when $\lambda =2$, and $a = 3$.\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:prediction}{{7}{11}{Solutions for LASSO, SCAD, and MCP for a single predictor when $\lambda =2$, and $a = 3$.\relax }{figure.caption.9}{}}
\citation{james2013introduction}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {section}{\numberline {4}Non-linear Models}{12}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Decision Tree Regression}{12}{subsection.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Example predictor data points of house values in Boston Suburbs\relax }}{12}{table.caption.10}\protected@file@percent }
\newlabel{tab:decision-tree-data}{{2}{12}{Example predictor data points of house values in Boston Suburbs\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Random Forests}{12}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Boosting}{12}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Support Vector Machines}{12}{subsection.4.4}\protected@file@percent }
\citation{james2013introduction}
\citation{james2013introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Decision tree of Boston suburbs median home value (in \$1000s)\relax }}{13}{figure.caption.11}\protected@file@percent }
\newlabel{fig:decision-tree}{{8}{13}{Decision tree of Boston suburbs median home value (in \$1000s)\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The decision line for a support vector machine maximizes the margin between itself and any observations. Image source: \cite  {james2013introduction}, page 340\relax }}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:svm-1}{{9}{13}{The decision line for a support vector machine maximizes the margin between itself and any observations. Image source: \cite {james2013introduction}, page 340\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Using a radial kernel to create a decision boundary. Image source: \cite  {james2013introduction}, page 353\relax }}{14}{figure.caption.13}\protected@file@percent }
\newlabel{fig:svm-2}{{10}{14}{Using a radial kernel to create a decision boundary. Image source: \cite {james2013introduction}, page 353\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluating Methods}{14}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Monte Carlo Data Generation}{14}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Choice of the Covariance Matrix}{15}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Independent Covariance}{15}{subsubsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Symmetric Compound}{15}{subsubsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Blockwise}{15}{subsubsection.5.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Unstructured Covariance}{16}{subsubsection.5.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.5}Autoregressive Covariance}{16}{subsubsection.5.2.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Factorial Design}{16}{subsection.5.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Choice of parameter values for the simulation study\relax }}{17}{table.caption.14}\protected@file@percent }
\newlabel{tab:simulation-parameters}{{3}{17}{Choice of parameter values for the simulation study\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Confusion Matrix}{17}{subsection.5.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Confusion Matrix\relax }}{17}{table.caption.15}\protected@file@percent }
\newlabel{tab:confusion_matrix}{{4}{17}{Confusion Matrix\relax }{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Bias}{18}{subsection.5.5}\protected@file@percent }
\bibstyle{plain}
\bibdata{references}
\bibcite{lumley2020leaps}{1}
\bibcite{breheny2016lasso}{2}
\bibcite{buehlmann2006boosting}{3}
\bibcite{capitaine2021random}{4}
\bibcite{fan2001variable}{5}
\bibcite{friedman2001elements}{6}
\bibcite{genuer2008random}{7}
\bibcite{hoerl1970ridge}{8}
\bibcite{james2017islr}{9}
\bibcite{james2013introduction}{10}
\bibcite{tibshirani1996regression}{11}
\bibcite{zhang2010nearly}{12}
\bibcite{zou2006adaptive}{13}
\bibcite{zou2005regularization}{14}
\gdef \@abspage@last{19}
