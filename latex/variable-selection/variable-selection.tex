\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[margin = 1in]{geometry}
\usepackage{listings}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{multirow}
\usepackage{url}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor=blue
}

% Allows us to use subfigures
\usepackage{caption}
\usepackage{subcaption}

\lstset{
	literate={~} {$\sim$}{1},
	belowcaptionskip=1\baselineskip,
	breaklines=true,
	frame=L,
	xleftmargin=\parindent,
	language=R,
	showstringspaces=false,
	basicstyle=\ttfamily
}

\pagestyle{fancy}
\fancyhead[L]{Variable Selection Techniques}
\fancyhead[R]{G. Ackall, C. Shrader}
\fancyfoot{}
\fancyfoot[C]{\thepage}

\setlength{\parskip}{6pt}

\title{Variable Selection Techniques}
\author{Gabriel Ackall and Connor Shrader}
\date{\today}

\newcommand{\argmin}[1]{\underset{\beta}{\text{arg min}}}
\newcommand{\sign}{\text{sign}}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
Linear regression is one of the simplest forms of statistical learning models. It assumes that some predictor variable $y$ can be modeled as a linear combination of a set of predictor variables $x_1, x_2, \dotsc, x_p$. More precisely, a linear regression model assumes that
\begin{equation}\label{eqn:linear-model}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{equation}
where $\beta_0, \beta_1, \dotsc, \beta_p$ are coefficients and $\epsilon$ is a random error with mean zero and variance $\sigma^2$. We also assume that this error has no correlation between different observations. When working with real data sets, the coefficient values are usually unknown; the goal of linear regression is to compute coefficient estimates $\hat{\beta_0}, \hat{\beta_1}, \dotsc, \hat{\beta_p}$ that are good estimates for the actual coefficients.

\subsection{Ordinary Least Squares}

Let $\beta = [\beta_0, \beta_1, \dotsc, \beta_p]^{T}$ be a vector of coefficient values, and let $\hat{\beta} = [\hat{\beta}_0, \hat{\beta}_1, \dotsc, \hat{\beta}_p]^{T}$ represent a vector of coefficient estimates. Let $\mathbf{X}$ be a $n\times(p + 1)$ matrix containing $n$ observations in $p$ variables. In order to account for the constant $\beta_0$ term in Equation \ref{eqn:linear-model}, we include an extra column in $\mathbf{X}$ with each entry equal to 1; the coefficient estimate for this additional ``variable'' will be $\hat{\beta}_0$. Finally, let $\mathbf{y}$ represent the vector of response values for each observation.

The most common way to compute $\hat{\beta}$ is with ordinary least squares. This method computes $\hat{\beta}$ by minimizing the residual sum of squares:
\begin{align}
	\hat{\beta}^{\text{OLS}} &= \argmin{\beta}\left\{\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2 \right\}\\
	&= \argmin{\beta} \left\{ (\mathbf{y} - \mathbf{X}\beta)^\top (\mathbf{y} - \mathbf{X}\beta)^\top\right\}
\end{align}
Ordinary least squares is favored for a few reasons. For one, the solution is very easy to compute; the coefficient estimate is given by
\begin{equation}
	\hat{\beta} = (\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}
\end{equation}
Another advantage of ordinary least squares comes from the Gauss-Markov Theorem. This theorem states that of all the linear unbiased estimators, ordinary least squares has the lowest variance. Alternative unbiased estimators will necessarily have a larger variance, which can lead to more inaccurate predictions.

\subsection{Linear Regression with High Dimensionality Data}

If ordinary least squares minimizes variance among all unbiased estimators, why would we use anything else? One issue is that the matrix $\mathbf{X}^\top\mathbf{X}$ is not always invertible. In fact, this matrix is never invertible in the case when $p>n$. Hence, in some situations, we cannot use ordinary least squares.

Even if the matrix is invertible, other methods may lead to better model performance. Despite having the least variance among unbiased estimators, ordinary least squares may still have high variance. Consequently, it may be worthwhile to sacrifice some bias in order to reduce variance.

Many biased linear regression models perform \textit{variable selection}, meaning that some coefficient estimates will be set to zero. In many applications, not all of the predictors will be related to the response, but ordinary least squares still attempts to use every predictor when computing coefficient estimates. Variable selection techniques can identify the predictors most strongly correlated to the data and eliminate the rest. This results in a simpler model that uses $p^\ast$ of the $p$ available predictors. This not only helps reduce variance, but it also creates more interpretable models.

The rest of this document examines many approaches to fitting linear regression models. These techniques fall under two categories: subset selection and penalized regression. Subset selection algorithms attempt to find a subset of predictors that is most strongly correlated to the response and  fits an ordinary least squares model to that subset. Penalized regression techniques punish large coefficient estimates, which encourages models that have smaller coefficient values. Depending on the particular algorithm used, penalized regression can also perform variable selection.

\subsection{Alternative Techniques with High Dimensionality Data}
In addition to penalized and subset selection linear regression methods used for variable selection, there are other techniques that can achieve similar results. One of these such methods includes using boosting. Boosting achieves accurate predictions by sequentially updating inaccurate models, each time correcting on the weaknesses of the predecessor. While boosting is traditionally used with decision trees and classification, its premise has shown success with linear models and regression, as demonstrated by Buehlmann et. al. \cite{buehlmann2006boosting}.

Random forest techniques have also shown promise and perform very well for high dimensionality data. However, they are limited in that they do not perform as well when the number of predictors is greater than the number of data samples. This is because random forests work by using the majority vote of thousands of decision trees on bootstrapped data. Due to the lack of samples, tree pruning can lead to increased error, and this can contribute to overfitting where the model fits the training data well, but cannot capture the true relationships of the data. This will lead to decreased accuracy when exposed to new testing data \cite{genuer2008random}. Still, random forest is a common model used when the number of predictors is greater than the number of samples and recently, some statisticians have proposed new random forest methods that perform much better than traditional methods \cite{capitaine2021random}.

\section{Subset Selection}
First, we discuss subset selection techniques. In general, these techniques are discrete algorithms that attempt to find a good model using only a subset of the available predictors.
\subsection{Best Subset Selection}
Best subset selection is a method for selecting the most influential predictors that minimize error in a least squares linear regression. It does this by fitting a linear regression model to every possible combination of predictors and then choosing the best model of all the possible combinations. The best model is determined through the use of a test error estimate. The most common examples of test error estimation indicators are Akaike information criterion (AIC), Bayesian information criterion (BIC), Adjusted $R^2$, and cross validation.

While best subset selection results in the best possible model given the predictors, it is very computationally expensive. As the number of predictors increases, the number of linear models that best subset selection has to fit increases exponentially. This can be seen in Table \ref{tab:subset-combinations}. Thus, for models with more than 40 predictors, this can become infeasible for most computers to compute \cite{james2013introduction}. Given that in many scenarios, especially those seen in medicine with genomic data or in scenarios where there are more predictors than data samples, there can be many thousands of predictors, and best subset selection becomes impossible.

\begin{table}[h!]
	\centering
	\caption{Number of fitted models depending on number of predictors (p)}
	\vspace{0.1in}
	\begin{tabular}{c|r@{\hskip 4pt}l}
		\hline
		p  &  \multicolumn{2}{c}{Fitted Models}\\
		\hline
		2   & $2^2$ & $=4$ \\
		10  & $2^{10}$ & $=1024$ \\
		100 & $2^{100}$ & $>10^{30}$ \\
		k   & $2^k$ & \\
	\end{tabular}
\label{tab:subset-combinations}
\end{table}

Figure \ref{fig:best-subset-selection} below demonstrates how the number of predictors can affect $R^2$ and BIC when using best subset selection. This plot was created by using the \lstinline!leaps! library (which provides a function to run best subset selection) \cite{lumley2020leaps}. We used the \lstinline!College! dataset provided by the \lstinline!ISLR! library \cite{james2017islr}, and fit linear regression models using \lstinline!Grad.Rate! as the response. We see that as the number of predictors increases, $R^2$ always increases (as expected). On the other hand, BIC is minimized with a moderate number of variables (between seven and nine). According to the BIC statistic, the best model has seven variables. The code used for this figure is in the \lstinline!r! folder of the REU GitHub repository.

\begin{figure}[!h]
	\centering
	\caption{$R^2$ and BIC when applying best subset selection}
	\label{fig:best-subset-selection}
	\includegraphics[width = 6in]{images/best-subset-selection.png}
\end{figure}

\subsection{Forward Stepwise Selection}
Forward stepwise selection aims to approximate the best combination of predictors in a linear regression model, but with a more computationally efficient method than best subset selection. Forward stepwise selection starts without using any predictors. It then slowly begins adding the most important predictor to the model. The predictor is chosen to minimize statistics such as p-value, AIC, BIC, or Adjusted $R^2$, to name a few. This is repeated until a stopping point is reached which can be defined by p-value, AIC, BIC, and more.

This process is much more computationally efficient than best subset selection, but it does not necessarily result in the best combination of parameters in the linear regression and is not guaranteed to result in the best model.

Figure \ref{fig:forward-stepwise-selection} shows the $R^2$ and BIC statistics when fitting models using forward stepwise selection. Again, we predicted \lstinline!Grad.Rate! using the \lstinline!College! data set using the \lstinline!leaps! library. The results are almost identical to what we saw for best subset selection. Even though the plots are similar, the specific model chosen by forward stepwise selection is actually different than the one found using best subset selection.

\begin{figure}[!h]
	\centering
	\caption{$R^2$ and BIC when applying forward stepwise selection}
	\label{fig:forward-stepwise-selection}
	\includegraphics[width = 6in]{images/forward-stepwise-selection.png}
\end{figure}

\subsection{Backward Stepwise Selection}
Backwards stepwise selection works very similarly to forward stepwise selection, except that it starts with every single predictor included in the least squares linear regression. Instead of adding predictors like in forward stepwise selection, backward stepwise selection removes the least important predictor in each iteration. Similar to the forward method, the importance of a predictor can be determined by its p-value, AIC, BIC, or Adjusted $R^2$. This is repeated until a pre-determined stopping point is reached.

Backward stepwise selection can often result in better models than forward stepwise selection because it is guaranteed to test all the predictors together. This is different from forward stepwise selection that can sometimes suppress predictors, especially those that are collinear. For these reasons, when its use is possible, backward stepwise selection is preferred to forward stepwise selection. However, in cases where the number of predictors are greater than the number of samples, backward stepwise selection is impossible. In these case, forward stepwise selection must be used.

Figure \ref{fig:backward-stepwise-selection} shows $R^2$ and BIC after applying backward stepwise selection to the \lstinline!College! data set. Again, the results are very similar to Figures \ref{fig:best-subset-selection} and \ref{fig:forward-stepwise-selection}, but the particular models chosen by the algorithm were slightly different. 

\begin{figure}[!h]
	\centering
	\caption{$R^2$ and BIC when applying backward stepwise selection}
	\label{fig:backward-stepwise-selection}
	\includegraphics[width = 6in]{images/backward-stepwise-selection.png}
\end{figure}

\subsection{Hybrid Stepwise Selection}
One weakness of the forward stepwise and backward stepwise methods is that they are greedy algorithms; in general, they will not find the best model for a given number of predictors. One way to improve model accuracy is to use hybrid stepwise selection, which allows for both forward steps and backward steps \cite{friedman2001elements}.

The algorithm could start with either zero predictors or all predictors. In each iteration, the method would either add a new predictor to the model or remove a predictor that does not increase performance. Like the forward and backward stepwise selection methods, this algorithm terminates when the model cannot be improved further; measuring the accuracy of the model can be determined using the AIC or BIC.

Although this strategy is slightly more computationally expensive than forward stepwise or backward stepwise selection, a hybrid approach may improve model results while still avoiding the unrealistic runtime of best subset selection.

\subsection{Forward Stagewise Selection}
One last method for feature selection is called forward stagewise regression. Like forward stepwise selection, forward stagewise selection starts by fitting a model using none of the predictors. In each iteration, the method chooses the predictor most closely correlated to the residuals of the current model, and fits a simple linear regression using the predictor against the residuals. The coefficient for this predictor in the simple model is then added to the corresponding coefficient in the other model. This process is repeated until none of the predictors are correlated with the residuals.

Note that in each iteration of this algorithm, only one of the coefficients is changed. As a result, this method has a long runtime. In the long run, forward stagewise selection is still competitive compared to the strategies previously discussed.

\section{Penalized Regression}
The following sections discuss several modifications to the ordinary least squares model that penalize large coefficient estimates. Recall that an ordinary least squares model assumes that some response variable $y$ can be modeled by

\begin{equation}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon
\end{equation}

where $x_1, x_2, \dotsc, x_p$ are predictors, $\beta_0, \beta_1, \dotsc, \beta_p$ are coefficients, and $\epsilon$ is some random error with mean 0 and variance $\sigma^2$. An ordinary least squares model estimates the coefficients as

\begin{equation}
	\hat{\beta}^{\text{OLS}} = \argmin{\beta}\left\{\sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2 \right\}
\end{equation}

In general, a penalized regression imposes some additional restriction that punishes large coefficient estimates. The coefficient estimates are then given by
\begin{equation}
	\hat{\beta} = \argmin{\beta}\left\{ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2 + \sum\limits_{j = 1}^p p(\beta_j) \right\}
\end{equation}
where $p(\beta_j)$ is some penalty function. Generally, this function should be large when the coefficient is large so that a model with smaller coefficients is favored. Penalized regression is especially useful when $p>n$ because the penalties can reduce variance and perform variable selection at the cost of some bias.

Because these models place penalties on large coefficient estimates, it is common to standardize the predictors to have a mean of 0 and a variance of 1. This ensures that none of the predictors have a disproportionate effect on the estimated coefficients.

\subsection{Ridge Regression}
Ridge regression helps to solve multicollinearity in predictors while also minimizing insignificant predictors \cite{hoerl1970ridge}. While it does not minimize these insignificant predictors completely to 0 and thus cannot be considered a variable selection method, it still proves very useful in large datasets.

Ridge regression works by minimizing Residual Sum Squared (RSS) plus a penalty as seen in Equation \ref{ridge_reg}. $\lambda$ is a tuning parameter and can be used to determine how much of an effect the penalty has on the regression. if $\lambda=0$, then the regression acts exactly like ordinary least squares regression, but if $\lambda \rightarrow \infty$, then $\beta_j \rightarrow 0$ and the regression line will be a horizontal line at the intercept, $\beta_0$.

\begin{equation}
	\hat{\beta}^{\text{ridge}} = \argmin{\beta}\left\{ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2  \right\}
	\label{ridge_reg}
\end{equation}

An alternative way to express ridge regression is with the equation
\begin{equation}
	\hat{\beta}^{\text{ridge}} = \argmin{\beta}\left\{ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2\right\}\quad\text{subject to}\quad \sum_{j=1}^{p} \beta_j^2\leq t
\end{equation}
for some tuning parameter $t$.

\subsection{Lasso Regression}

The least absolute shrinkage and selection operation, often referred to as \textit{lasso}, is a shrinkage method with a very similar form to lasso regression \cite{tibshirani1996regression, james2017islr, james2013introduction}. The coefficient estimates satisfy
\begin{equation}
	\hat{\beta}^{\text{lasso}}=\argmin{\beta}\left\{ \sum\limits_{i = 1}^n \left( y_i - \beta_0 - \sum\limits_{j = 1}^p \beta_j x_{ij} \right) + \lambda\sum\limits_{j = 1}^p \vert \beta_j \vert \right\}
\end{equation}
If $\lambda = 0$, then the lasso model is equivalent to the ordinary least squares model; if $\lambda \to \infty$, then the coefficients for all predictors will be set to 0. An equivalently way to define lasso regression is by
\begin{equation}
	\hat{\beta}^{\text{lasso}} = \argmin{\beta}\left\{ \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_jx_{ij} \right)^2 \right\}\quad\text{subject to}\quad \sum_{j=1}^{p} \vert \beta_j \vert\leq t
\end{equation}
where $t$ is a tuning parameter.

One useful property of the lasso method is that it can perform variable selection by setting some coefficients to zero. To understand why lasso regression can perform variable selection whereas ride regression cannot, consider Figure \ref{fig:ridge-lasso} below. This figure demonstrates the case when $p=2$ and $t = 1$. The red diamond on the left represents the condition $\vert \beta_1 \vert + \vert \beta_2 \vert < 1$ for ridge regression, while the circle on the right represents the condition $\beta_1^2 + \beta_2^2 < 1$ for lasso regression. The black curves represent contours of the residual sum of squares error for values of $\beta_1$ and $\beta_2$. The black point in the center of these curves is where the RSS is minimized; this represents the values of $\beta_1$ and $\beta_2$ that would be selected by ordinary least squares.

\begin{figure}[!h]
	\centering
	\caption{Error and constant curves for the lasso and ridge models when $p=2$.}
	\label{fig:ridge-lasso}
	\includegraphics[width = 0.6\textwidth]{images/ridge-lasso.png}
\end{figure}

In the left plot, the intersection of the black curve and the red diamond represents the parameter values chosen by lasso regression; this point minimizes the RSS under the condition $\vert \beta_1 \vert + \vert \beta_2 \vert < 1$. Because the red region is a square, this intersection occurs at a point where $\beta_1 = 0$; hence, the lasso method removes the predictor $\beta_1$. On the other hand, the circular shape of the constrained region for ridge regression cannot perform variable selection because the intersection does not occur at one of the axes.

The lasso method is particularly useful in the case where $p>n$ because of its ability to select variables; a model with fewer variables has less variance and is more interpretable. One major downside of lasso regression is that it does not handle multicollinearity as nicely as ridge regression. Another downside of lasso regression is that it does not have a closed-form solution, which can lead to instability in the model. This can be demonstrated by the greater spread of $\beta$ coefficients compared to ridge regression, as visible in Figure \ref{fig:lasso-and-ridge-instability}.

\begin{figure}[!ht]
	\centering
	\caption{Instability of Lasso and Ridge Regression to Changes in Training Data}
	\includegraphics[width = 0.7\linewidth]{images/lasso-vs-ridge-instability.png}
	\label{fig:lasso-and-ridge-instability}
\end{figure}

\subsection{Elastic Net Regression}
Elastic net regression serves as a combination between ridge and lasso regression. It can handle  multicollinearity as well as perform variable selection. The coefficients for elastic net regression can be determined by 
\begin{equation}
	\hat{\beta}^{\text{ENet}}=\argmin{\beta}\left\{ \sum\limits_{i = 1}^n \left( y_i - \beta_0 - \sum\limits_{i = 1}^p \beta_j x_{ij} \right) + \lambda_2\sum\limits_{j = 1}^p \beta_j^2 + \lambda_1\sum\limits_{j = 1}^p \vert \beta_j \vert \right\}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are both tuning parameters to be determined later.

An important limitation to note is that elastic net performs best when it close to either ridge or lasso regression, meaning that either $\lambda_1 >> \lambda_2$ or vice versa \cite{zou2005regularization}. Additionally, because elastic net requires two tuning parameters, this makes it much more difficult to determine the best combination of tuning parameters to minimize error in the regression. However, this problem has been largely solved through by the LARS-EN algorithm developed by Zou et. al. which efficiently solves for the tuning parameters.
% I will add more here later

\subsection{Adaptive Lasso Regression}
Normally in lasso regression, each predictor is weighted the same in the penalty function. Adaptive lasso regression is different in that a weight, $\hat{w}_j$ is multiplied to the penalty function. The coefficients for adaptive lasso regression as designed by Zou et. al. \cite{zou2006adaptive} can be defined by
\begin{equation}
	\hat{\beta}^{\text{adaptive}}=\argmin{\beta}\left\{ \sum\limits_{i = 1}^n \left( y_i - \beta_0 - \sum\limits_{i = 1}^p \beta_j x_{ij} \right) + \lambda\sum\limits_{j = 1}^p \hat{w}_j\vert \beta_j \vert \right\}
\end{equation}
where $\lambda$ is a tuning parameter to be determined later and $\hat{w}_j$ is defined as $\frac{1}{\vert\hat{\beta}\vert^{\gamma}}$ with $\gamma$ being a chosen parameter greater than 0.

Because of the weight that is implemented in adaptive lasso regression, zero-coefficients have a weight that is inflated up to infinity, and thus are punished much more harshly than large coefficients whose weight is much smaller in comparison. This is a similar rationale to SCAD and helps to reduce some of the bias from lasso regression. Bridge regression is the general form of lasso regression from which adaptive lasso originates from. When $\gamma < 1$, bridge regression as shown in Equation \ref{adap_lasso} is not continuous, which results in model prediction instability. However, adaptive lasso regression is completely continuous and thus has much more consistent coefficients when fitted.
% I will add more here in the future

\begin{equation}
	\hat{\beta}^{\text{lasso}}=\argmin{\beta}\left\{ \sum\limits_{i = 1}^n \left( y_i - \beta_0 - \sum\limits_{i = 1}^p \beta_j x_{ij} \right) + \lambda\sum\limits_{j = 1}^p \vert \beta_j \vert ^\gamma \right\}
	\label{adap_lasso}
\end{equation}


\subsection{Smoothly Clipped Absolute Deviation Regression}
One major flaw of the lasso method is that the penalty punishes large coefficients, even if those coefficients should be large. One way to modify the lasso method is to use the \textit{smoothly clipped absolute deviation} (SCAD) penalty \cite{fan2001variable}. The goal of this method is to punish large coefficients less severely, which can help mitigate some of the bias introduced by the lasso method.
\begin{equation}
	\hat{\beta}^{\text{SCAD}}=\argmin{\beta}\left\{ \sum\limits_{i = 1}^n \left( y_i - \beta_0 - \sum\limits_{i = 1}^p \beta_j x_{ij} \right) + \lambda\sum\limits_{j = 1}^p J_a(\beta_j, \lambda) \right\}
\end{equation}
Here, $J_a(\beta, \lambda)$ is a penalty function that satisfies
\begin{equation}
	\frac{dJ_a(\beta, \lambda)}{d\beta} = \lambda\cdot\sign(\beta)\left[ I(\vert \beta \vert<\lambda) + \frac{(a\lambda - \vert \beta\vert)_+}{(a - 1)\lambda}I(\vert \beta \vert > \lambda) \right]
\end{equation}
where $\lambda \geq 0$ and $a\geq 2$ are tuning parameters. An equivalent way to write this is
\begin{equation}
	\frac{dJ_a(\beta, \lambda)}{d\beta}\left\{\begin{array}{ll}
		\lambda,&\vert \beta \vert\leq \lambda\\
		\frac{a\lambda - \vert \beta \vert}{a - 1},&\lambda < \vert \beta \vert < a\lambda\\
		0,&\alpha\lambda < \vert \beta \vert
	\end{array}\right.
\end{equation}
This penalty function does not punish coefficients with large magnitude as heavily as the lasso method. In fact, if the magnitude of a coefficient is larger than $a\lambda$, then the penalty becomes constant. See Figure \ref{fig:penalty} for a plot of the SCAD penalty as a function of the coefficient value.

Integrating with respect to $\beta$ \cite{breheny2016lasso}, we see that
\begin{equation}
	J_a(\beta, \lambda) = \left\{\begin{array}{ll}
		\lambda \vert \beta \vert,&\vert \beta \vert \leq \lambda\\
		\frac{2a\lambda\vert\beta\vert - \beta^2-\lambda^2}{2(a - 1)},&\lambda < \vert \beta \vert < a\lambda\\
		\frac{\lambda^2(a + 1)}{2},&a\lambda < \vert \beta \vert
	\end{array}\right.
\end{equation}

\subsection{Minimax Concave Penalty Regression}
The minimax concave penalty (MCP) method is very similar to smoothly clipped absolute deviation \cite{zhang2010nearly, breheny2016lasso}. Both methods are used to avoid the high bias caused by the lasso method. MCP uses a penalty function that satisfies
\begin{equation}
	\frac{dJ_a(\beta, \lambda)}{d\beta} = \left\{\begin{array}{ll}
		\sign(\beta)\left(\lambda - \frac{\vert \beta \vert}{a}\right),& \vert \beta \vert \leq a\lambda\\
		0,&a\lambda < \vert \beta \vert
	\end{array}\right.
\end{equation}
where $\lambda\geq 0$ and $a> 0$ are tuning parameters. Integrating \cite{breheny2016lasso}, we see that
\begin{equation}
	J_a(\beta, \lambda) = \left\{\begin{array}{ll}
		\lambda \vert \beta \vert - \frac{\beta^2}{2a},&\vert \beta \vert \leq a\lambda\\
		\frac{1}{2}a\lambda^2,&a\lambda < \vert \beta \vert
	\end{array}\right.
\end{equation}

Figure \ref{fig:lasso-scad-mcp} below shows the penalty functions (and their derivatives) for LASSO, SCAD, and MCP as a function of a coefficient value $\beta$. We see that LASSO applies a much stronger penalty to large coefficients than SCAD or MCP. Also, note that SCAD starts with a derivative equal to that of the lasso for small values of $\beta$; on the other hand, the derivative of the penalty function for MCP starts decreasing immediately.

\begin{figure}[!h]
	\caption{Penalty functions for LASSO, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 2$ and $a = 3$.}
	\label{fig:lasso-scad-mcp}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\caption{Penalty functions for LASSO, SCAD, and MCP}
		\label{fig:penalty}
		\includegraphics[width=\textwidth]{images/penalty.png}
	\end{subfigure}
	\hspace{30pt}
	\begin{subfigure}[b]{0.4\textwidth}
		\caption{Derivatives of the penalty functions for LASSO, SCAD, and MCP}
		\label{fig:derivative}
		\includegraphics[width=\textwidth]{images/derivative.png}
	\end{subfigure}
\end{figure}

Now, consider the case where $p=1$ (there is only one predictor). Figure \ref{fig:prediction} shows the solutions given when using LASSO, SCAD, and MCP on such a model. The $x$-axis gives the actual coefficient for the single variable, and the $y$-axis represents the coefficient estimate produced using each of the algorithms. We used the particular values $\lambda = 2$ and $a = 3$. The gray line is the identity function, which also equals the solution obtained using ordinary least squares.

We see that all three methods set the predicted value to zero when the actual coefficient is small. Also, note that the LASSO method is always off from the identity function when the coefficient is large. On the other hand, SCAD and MCP merge with the identity function when the coefficient is sufficiently large. This shows that both SCAD and MCP can avoid the high bias that LASSO introduces.

\begin{figure}[!h]
	\centering
	\caption{Solutions for LASSO, SCAD, and MCP for a single predictor when $\lambda=2$, and $a = 3$.}
	\label{fig:prediction}
	\includegraphics[width = 0.4\textwidth]{images/prediction.png}
\end{figure}

\section{Non-linear Models}
We next discuss several non-linear methods for regression and classification.
\subsection{Decision Tree Regression}
A decision tree is a machine learning algorithm of nodes and branches in a tree-like structure. They work from top to bottom by iteratively splitting into groups based on input values in order to minimize residual sum squared. This creates a flowchart that starts from the top and can be followed down. The direction of movement is determined by a boolean condition on the node.

An example decision tree can be seen in Figure \ref{fig:decision-tree}. An example row of predictor data points can be seen in Table \ref{tab:decision-tree-data}. In this illustration, if the condition on the node is true, then the left direction is taken, and if the condition is false, then the right direction is taken. For these example data points, the decision tree predicts a median housing value of \$20,630, which is slightly over the correct value of \$18,900.

Decision trees have many other uses and can be used for both classification and regression. Additionally, decision trees can prune branches that use a variable with low importance. This can serve as a type of variable selection and helps to reduce overfitting as well as the complexity of the model. This will inherently cause some bias, but because of the reduction in variance, overall error is decreased.

\begin{table}[!ht]
	\centering
	\caption{Example predictor data points of house values in Boston Suburbs}
	\label{tab:decision-tree-data}
	\begin{tabular}{|l|l|l|l|l|l|l}
		\hline
		\begin{tabular}[c]{@{}l@{}}crim\\ (crime per \\ capita)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}zn\\ (lot zone \\ proportion)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}indus\\ (proportion \\ of non-retail \\ business)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}chas\\ (bounds \\ Charles River)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}nox\\ (nitrogen \\ oxides\\ concentration)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}rm\\ (rooms per\\ dwelling)\end{tabular} &
		\multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}age\\ (proportion of \\ units built \\ before 1940)\end{tabular}} \\ \hline
		0.11747 & 12.5  & 7.87   & 0    & 0.524 & 6.009 & \multicolumn{1}{l|}{82.9} \\ \hline
		\begin{tabular}[c]{@{}l@{}}tax\\ (tax rate)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}black\\ (proportion\\ of black \\ population)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}dis\\ (distance to\\ employment \\ centers)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}ptratio\\ (pupil-teacher\\ ratio)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}rad\\ (accessibility\\ to radial\\ highways)\end{tabular} &
		\begin{tabular}[c]{@{}l@{}}lstat\\ (lower \\ status of\\ population)\end{tabular} &
		\\ \cline{1-6}
		311     & 396.9 & 6.2267 & 15.2 & 5     & 13.27 &                           \\ \cline{1-6}
	\end{tabular}
\end{table}
\begin{figure}[!ht]
	\centering
	\caption{Decision tree of Boston suburbs median home value (in \$1000s)}
	\includegraphics[width = 0.6\linewidth]{Images/decision-tree.png}
	\label{fig:decision-tree}
\end{figure}



\subsection{Random Forests}
One of the flaws of decision tree models is that they generally have high variance. A small change in some of the data points can completely change the tree's structure. Random forest models can overcome the high variance of decision tree models by aggregating the predictions of many separate trees. Each individual tree is fitted using only a subset of the available observations and predictors. This means that each tree will be significantly different from any of the other trees. To make a prediction with a random forest model, the predictions of each individual tree are calculated and aggregated (usually using the mean for regression and the mode for classification).

By only considering a subset of the predictors for each tree, there will be relatively little correlation different trees. In the case where all $p$ predictors are used for each tree (which is called bagging), many of the trees will have similar structures because they all use the predictors most correlated with the response. This can cause high variance. In many random forest models, it is common to use $\sqrt{p}$ predictors for each individual tree.

\subsection{Gradient Boosting Model}
Boosting is the technique of sequentially improving a weak learner until it becomes a strong learner. A gradient boosting model (GBM) is a boosting technique that uses gradient descent to minimize error in a model and correct the shortcomings of the previous weak learner model. This is done through fitting a model, whether that is a linear regression or decision tree model, to a set of data points. From there, the residual of each data point is calculated and another linear regression or decision tree model is fitted to those residuals. A new model is fitted to the residual data of the residual data fitted model, and so on. These models are then all added together to result in a strong GBM model.

GBMs can be used for both regression and classification if they use decision trees, or if they use linear regression models, they can only perform regression. When using a GBM with decision trees, variable importance and pruning can be used as a sort of pseudo-variable selection method to lower complexity and prevent over-fitting. However, when using a GBM with linear regression, there is the ability to use lasso, ridge, and elastic net penalized regression as the weak learner and perform variable selection.

\subsubsection{XGBoost}
GBMs often suffer from slow computation speeds due to the large number of sequential models that need to be trained. Extreme Gradient Boosting (XGBoost) is a faster version of GBM that utilizes parallel computing as well as different optimization techniques to speed up computation. For these reasons, XGBoost is often preferred over standard GBMs and is very commonly used in many machine learning applications.

\subsection{Support Vector Machines}
Support vector machines are versatile statistical models; they can be used for both regression and classification. In the most basic case, a support vector machine is a binary classier whose decision boundary is a $(p-1)$-dimensional hyperplane in $p$-dimensional space. This hyperplane separates all of the observations for one class on one side, and the observations for the other class lie on the opposite side. Moreover, the hyperplane chosen by a support vector machine will maximize the distance between this hyperplane and any of the observation points. Figure \ref{fig:svm-1} demonstrates the line chosen by a support vector machine in the case where there are two predictors.

\begin{figure}[!h]
	\centering
	\caption{The decision line for a support vector machine maximizes the margin between itself and any observations. Image source: \cite{james2013introduction}, page 340}
	\label{fig:svm-1}
	\includegraphics[width = 3in]{images/svm-1.png}
\end{figure}

This description of a support vector machine is simple but incomplete. Most datasets are not able to be split perfectly into two sides like the example shown in \ref{fig:svm-1}. Furthermore, this model has very high variance changing the points near the hyperplane can significantly alter the hyperplane. Finally, this model cannot handle cases where the true boundary is non-linear.

Luckily, support vector machines in practice can handle such issues. Typically, support vector machines are allowed to misclassify some of the training data, which address the cases where the data cannot be split perfectly by a hyperplane. Also, the predictor space can be enlarged to handle non-linear decision boundaries; this is typically done by using \textbf{kernels}. For example, using a radial kernel can create decision boundaries that enclose regions of the $p$-dimensional space. Figure \ref{fig:svm-2} shows an example of this.

\begin{figure}[!h]
	\centering
	\caption{Using a radial kernel to create a decision boundary. Image source: \cite{james2013introduction}, page 353}
	\label{fig:svm-2}
	\includegraphics[width = 3in]{images/svm-2}
\end{figure}

Support vector machines can be generalized even further to handle more than two classes, or even regression.
\section{Evaluating Methods}
\subsection{Monte Carlo Data Generation}
Using Monte Carlo simulations to generate data is extremely useful for evaluating linear regression models and comparing them against each other. Generated data is beneficial because it allows us to know the true coefficient values, which allows accuracy, bias, and more to be calculated. It additionally allows for control of the data and situations. For example, this can be used to test different models in environments where predictors are completely independent, which is impossible with real data, or on the opposite spectrum when predictors are heavily correlated with each other. This can allow for the testing of model strengths and weaknesses. Because datasets can be generated thousands of times, Monte Carlo simulations can be used to evaluate randomness and variation of different models.

Data with $n$ samples from $p$ predictors is created through Monte Carlo simulations by first defining the $(p + 1)\times 1$ vector $\beta = [\beta_0, \beta_1, \dotsc, \beta_p]^\top$ of coefficient values.

Next, a $n\times(p + 1)$ matrix $\mathbf{X}$ is generated such that the entries of the first column are all 1 and the remaining $n\times p$ submatrix is distributed according to the $p$-dimensional multivariate normal distribution $\mathcal{N}_p(0, \mathbf{\Sigma})$. Entries have mean zero and covariance determined by the covariance matrix $\mathbf{\Sigma}$. The next section discusses different choices for the covariance matrix and how they can reflect real data.

Finally, a $n\times 1$ error vector $E\sim \mathcal{N}(0, \sigma^2)$ is generated with mean zero and variance $\sigma^2$. This error term reflects the random error for the general linear model from equation \ref{eqn:linear-model}.

The response $\mathbf{y}$ can then be computed as
\begin{equation}
	\mathbf{y}=\mathbf{X}\beta + E
\end{equation}
We can then fit statistical models using the predictor variables from $\mathbf{X}$ and the corresponding response $\mathbf{y}$. This process can then be repeated many times to evaluate model effectiveness.

\subsection{Choice of the Covariance Matrix}
Recall that data generated using Monte Carlo simulations is distributed according to the $p$-dimensional multivariate normal distribution $\mathcal{N}_p(0, \mathbf{\Sigma})$. The choice for the covariance matrix $\mathbf{\Sigma}$ can have a significant effect on the performance of statistical models. Here, we describe several different types of covariance matrices.

\subsubsection{Independent Covariance}
Independent covariance assumes that the covariance matrix is diagonal; that is, it can be written as
\begin{equation}
	\mathbf{\Sigma} = \begin{bmatrix}
		\sigma_1^2 & 0 & \cdots & 0\\
		0 & \sigma_2^2 & \cdots & 0\\
		\vdots & \vdots & \ddots & \vdots\\
		0 & 0 & \cdots & \sigma_p^2
	\end{bmatrix}
\end{equation}
where $\sigma_i^2$ is the variance of predictor $i$. Because $\mathbf{\Sigma}_{ij}$ whenever $i\neq j$, each pair of predictors are independent.

An independent covariance matrix is easy to use since it is equivalent to just taking $p$ single-variable normal distributions. However, the assumption that each pair of variables is independent is unrealistic; in most real-world situations, there is some correlation between different variables. As such, considering other covariance matrices can lead to results that better reflect real-world data.

\subsubsection{Symmetric Compound}
With a symmetric compound covariance matrix, we assume that $\mathbf{\Sigma}$ takes the form
\begin{equation}
	\mathbf{\Sigma} = \begin{bmatrix}
		\sigma_1^2 & \rho & \cdots & \rho\\
		\rho & \sigma_2^2 & \cdots & \rho\\
		\vdots & \vdots & \ddots & \vdots\\
		\rho & \rho & \cdots & \sigma_p^2
	\end{bmatrix}
\end{equation}
where $\sigma_i^2$ is the variance of predictor $i$ and $\rho \in (0, 1)$ is the covariance between every pair of predictors. This type of covariance is more general than independent covariance, and it can also be more realistic. However, in most real-world scenarios, the correlations between pairs of parameters will likely not be the same.

\subsubsection{Blockwise}
In a blockwise covariance distribution, we assume that $\mathbf{\Sigma}$ is can be written as a block-diagonal matrix
\begin{equation}
	\mathbf{\Sigma} = \begin{bmatrix}
		S_1 & O & \cdots & O\\
		O & S_2 & \cdots & O\\
		\vdots & \vdots & \ddots & \vdots\\
		O & O & \cdots & S_k
	\end{bmatrix}
\end{equation}

Blocks denoted as $O$ are zero blocks. Each block $S_l$ has the form
\begin{equation}
	S_l = \begin{bmatrix}
		\sigma_{r}^2 & \rho & \cdots & \rho\\
		\rho & \sigma_{r + 1}^2 & \cdots & \rho\\
		\vdots & \vdots & \ddots & \vdots\\
		\rho & \rho & \cdots & \sigma_{s}^2
	\end{bmatrix}	
\end{equation}
where $\sigma_i^2$ is the variance of predictor $i$ and $\rho\in (0, 1)$ is the covariance of predictors in the same block.

\subsubsection{Unstructured Covariance}
Finally, unstructed covariance assumes that $\mathbf{\Sigma}$ takes no structured form. We have
\begin{equation}
	\mathbf{\Sigma} = \begin{bmatrix}
		\sigma_1^2 & \sigma_{12} & \cdots & \sigma_{1p}\\
		\sigma_{21} & \sigma_2^2 & \cdots & \sigma_{2p}\\
		\vdots & \vdots & \ddots & \vdots \\
		\sigma_{p1} & \sigma_{p2} & \cdots & \sigma_p^2
	\end{bmatrix}	
\end{equation}
where $\sigma_i^2$ is the variance of predictor $i$ and $\sigma_{ij}$ is the covariance of predictors $i$ and $j$. The covariance matrix is always symmetric, so $\sigma_{ij} = \sigma_{ji}$.

\subsubsection{Autoregressive Covariance}
Autoregressive covariance (also called $\text{AR}(1)$), assumes that
\begin{equation}
	\mathbf{\Sigma} = \begin{bmatrix}
		\sigma_1^2 & \rho & \cdots & \rho^{p - 1}\\
		\rho & \sigma_2^2 & \cdots & \rho^{p - 2}\\
		\vdots & \vdots & \ddots & \vdots\\
		\rho^{p - 1} & \rho^{p - 2} & \cdots & \sigma_p^2
	\end{bmatrix}
\end{equation}
where $\sigma_i^2$ is the variance of predictor $i$ and $\rho \in (0, 1)$. In general, for $i\neq j$, we have $\mathbf{\Sigma}_{ij} = \rho^{\vert i - j\vert}$. This implies that predictors that are near each other are more correlated than predictors far apart. AR(1) is a very useful type of covariance matrix because it is not as simple as independent or symmetric compound covariance. AR(1) is particularly useful when used for applications such as time-series analysis, since nearby predictors are usually close together in time. Consequently, it makes sense for predictors close in time to have the highest correlation.

\subsection{Factorial Design}
Using Monte Carlo simulations, we can study the effectiveness of regression models using a factorial design. For each simulation, we have control over parameters such as the number of predictors, the correlation of predictors, and the amount of random error. For each parameter, we chose two or three different levels. our simulations  go through every possible combination of these parameter values.

Below is a list of the parameters we tuned and the values that we used:
\begin{itemize}
	\item $n$, the number of observations.
	\item $p$, the number of predictors.
	\item $p^\ast$, the number [proportion?] of predictors that are non-zero.
	\item $\sigma$, the variance of the random error in each observation.
	\item The type of covariance among the predictors [do we include blockwise and unstructured?].
	\item $\rho$, the covariance between predictors (the exact meaning of $\rho$ depends on the type of covariance used).
\end{itemize}

Table \ref{tab:simulation-parameters} shows the values chosen for each parameter in this study.
\begin{table}[h]
	\caption{Choice of parameter values for the simulation study}
	\label{tab:simulation-parameters}
	\begin{tabular}{l|lll}
		Parameter       & Value 1     & Value 2            & Value 3        \\ \hline
		$n$             & 50          & 200                & 1000           \\
		$p$             & 10          & 100                & 2000           \\
		$p^\ast$        & ?           & ?                  & ?              \\
		$\sigma$        & 1           & 3                  & 6              \\
		Covariance type & Independent & Symmetric Compound & Autoregressive \\
		$\rho$          & 0.2         & 0.5                & 0.9           
	\end{tabular}
\end{table}

Because of the factorial design, every possible combination of these parameters will be considered.

\subsection{Confusion Matrix}
In classification, a confusion matrix, as visible in Table \ref{tab:confusion_matrix}, can tell us how well a model classifies different objects. For regression, there is no classification, and so a confusion matrix cannot be used in its traditional way. However, it can be used to identify how well a variable selection linear regression model chooses its predictors. In this case, it can be viewed as classification of whether the model chooses the predictor or not, compared to the truth of whether the predictor has a coefficient $> 0$ or is not significant and thus is 0. This results in 4 possibilities for each model coefficient: True Positive meaning that the model correctly included the predictor, True Negative meaning that the model correctly didn't include the predictor, False Positive meaning that the model incorrectly included the predictor, and False Negative meaning that the model incorrectly didn't include the predictor.


The values in a confusion matrix can be used to calculate a variety of different statistics. Some of the most common of these statistics are accuracy, sensitivity, and specificity. The importance of each statistic depends on the scenario, but the higher the accuracy, sensitivity, or specificity; the better.

\begin{table}[h!]
	\centering
	\begin{tabular}{llll}
		&                       & \multicolumn{2}{c}{Predicted Condition}                       \\ \cline{3-4} 
		& \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{Positive} & \multicolumn{1}{l|}{Negative} \\ \cline{2-4} 
		\multicolumn{1}{c|}{\multirow{2}{*}{True Condition}} & \multicolumn{1}{l|}{Positive} & \multicolumn{1}{l|}{True Positive (TP)} & \multicolumn{1}{l|}{False Negative (FN)} \\ \cline{2-4} 
		\multicolumn{1}{c|}{}                                & \multicolumn{1}{l|}{Negative} & \multicolumn{1}{l|}{False Positive (FP)} & \multicolumn{1}{l|}{True Negative (TN)}  \\ \cline{2-4} 
	\end{tabular}
	\caption{Confusion Matrix}
	\label{tab:confusion_matrix}
\end{table}

Accuracy represents the total percentage of classifications that are classified correctly. This is by far the most commonly used statistic and can be calculated by
\begin{equation}
	\text{Accuracy (ACC)} = \frac{TP + TN}{Total} \,.
\end{equation}

Sensitivity is the true positive rate. In other words, it is the rate of True Positive classifications against the total amount of actual Positive classifications. Sensitivity is often very important in tests for a disease or illness. This is because a patient can always be tested again if their positive result is false, but if a positive patient is mistakenly diagnosed as negative, the illness can go undetected and can lead to injuries or mortality. In these scenarios, specificity will often be sacrificed for increased sensitivity. Sensitivity can be calculated by
\begin{equation}
	\text{Sensitivity (SEN)} = \frac{TP}{TP + FN} \, .
\end{equation}

Specificity is the true negative rate. It is the rate of True Negative classifications against the total number of actual Negative classifications. Specificity is very important when False Positive results are very detrimental. In many cases, False Positive results are not as detrimental as False Negatives and thus specificity is often traded for sensitivity. However, in cases such as the court system, having a very high specificity is important so that innocent people are not wrongly convicted of crimes they did not commit. Specificity, also known as true negative rate, can be calculated by
\begin{equation}
	\text{Specificity (TNR)} = \frac{TN}{TN + FP} \, .
\end{equation}

Another commonly used confusion matrix statistic is F-score. F-score also helps measure the accuracy of classifications. It is determined from the precision and sensitivity of a model and helps to portray the balance between the two. Precision, also known as positive predictive value, can be calculated by
\begin{equation}
	\text{Precision (PPV)} = \frac{TP}{TP + FP} .
\end{equation}

With the equation for precision and sensitivity, the F-score can be calculated and simplified to
\begin{equation}
	F_1 = \frac{2 \cdot PPV \cdot SEN}{PPV + SEN} = \frac{2 \cdot TP}{2 \cdot TP + FP + FN}\, .
\end{equation}

\subsection{Bias}
Bias is the difference between the estimated value of a coefficient and its true value. Because it necessitates the true value of a coefficient, bias can only be calculated when data is generated using Monte Carlo methods. Bias can be calculated as a function of the euclidean distance between the predictor coefficients through the equation

\begin{equation}
	\text{bias} \text{(model)} = \sqrt{\sum_{i=1}^{p}\left( \beta_i - \hat{\beta}_i \right)^2} \, .
\end{equation}

Variable selection linear regression methods increase bias, but still lead to overall decreases in error because they decrease error from variance. This trade-off must be carefully balanced and depends greatly on the environment of the data.
%can add a graphic about the variance-bias tradeoff, once we find out how to calculate that


\newpage
\bibliographystyle{plain}
\bibliography{references}
\end{document}