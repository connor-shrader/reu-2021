\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nielsen2016tree}
\citation{nielsen2016tree}
\providecommand \oddpage@label [2]{}
\citation{james2013introduction}
\citation{friedman2001elements}
\citation{akaike1998information}
\citation{schwarz1978estimating}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{zou2005regularization}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{breiman2001random}
\citation{chen2021xgboost}
\citation{cortes1995support}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{akaike1998information}
\citation{schwarz1978estimating}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{3}{section.2}\protected@file@percent }
\newlabel{sec:methodology}{{2}{3}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Subset Selection Methods}{3}{subsection.2.1}\protected@file@percent }
\citation{james2013introduction}
\citation{hoerl1970ridge}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Penalized Regression}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:penalized-regression-lambda}{{3}{4}{Penalized Regression}{equation.2.3}{}}
\citation{tibshirani1996regression}
\citation{james2013introduction}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{fan2001variable}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\newlabel{eqn:scad-derivative-indicator}{{5}{6}{Penalized Regression}{equation.2.5}{}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ridge-diagram}{{1a}{6}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.1}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{6}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.1}{}}
\newlabel{fig:lasso-diagram}{{1b}{6}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.1}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{6}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region in each plot represents the coefficient values selected by lasso and ridge.\relax }}{6}{figure.caption.1}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{1}{6}{RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region in each plot represents the coefficient values selected by lasso and ridge.\relax }{figure.caption.1}{}}
\citation{breheny2016lasso}
\citation{friedman2010regularization}
\citation{breheny2011ncvreg}
\citation{fan2001variable}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{fig:penalty}{{2a}{7}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:penalty}{{a}{7}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.2}{}}
\newlabel{fig:derivative}{{2b}{7}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.2}{}}
\newlabel{sub@fig:derivative}{{b}{7}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{2}{7}{Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.2}{}}
\newlabel{ols-orthonormal-solution}{{10}{7}{Penalized Regression}{equation.2.10}{}}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{donoho1994ideal}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input ($z$) represents an entry from the vector $\mathbf  {z}=\mathbf  {X}^\top \mathbf  {y}$; the output ($\hat  {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{8}{figure.caption.3}\protected@file@percent }
\newlabel{fig:prediction}{{3}{8}{Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input ($z$) represents an entry from the vector $\mathbf {z}=\mathbf {X}^\top \mathbf {y}$; the output ($\hat {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.3}{}}
\citation{breiman2001random}
\citation{efron1994introduction}
\citation{breiman1996bagging}
\citation{schapire1990strength}
\citation{friedman2001greedy}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Non-linear models}{9}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualization of how predictions are made using a random forest model. An observation is input into each decision tree. Predictions from each tree are then aggregated into a single result that is used as the final prediction.\relax }}{9}{figure.caption.4}\protected@file@percent }
\newlabel{fig:random-forest}{{4}{9}{Visualization of how predictions are made using a random forest model. An observation is input into each decision tree. Predictions from each tree are then aggregated into a single result that is used as the final prediction.\relax }{figure.caption.4}{}}
\citation{chen2016xgboost}
\citation{cortes1995support}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces How a boosting model with decision trees is fitted. Each tree is fitted to correct the errors of the previous tree. Predictions are made by combining the results from each decision tree.\relax }}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:boosting}{{5}{10}{How a boosting model with decision trees is fitted. Each tree is fitted to correct the errors of the previous tree. Predictions are made by combining the results from each decision tree.\relax }{figure.caption.5}{}}
\citation{drucker1997support}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualization of a simple support vector machine for binary classification. The solid black line is the hyperplane that maximizes the margin between the two classes. The points with a black outline are the support vectors. They are the points that define the hyperplane.\relax }}{11}{figure.caption.6}\protected@file@percent }
\newlabel{fig:svm}{{6}{11}{Visualization of a simple support vector machine for binary classification. The solid black line is the hyperplane that maximizes the margin between the two classes. The points with a black outline are the support vectors. They are the points that define the hyperplane.\relax }{figure.caption.6}{}}
\bibstyle{plain-lf}
\bibdata{references}
\bibcite{akaike1998information}{1}
\bibcite{breheny2016lasso}{2}
\bibcite{breheny2011ncvreg}{3}
\bibcite{breiman1996bagging}{4}
\bibcite{breiman2001random}{5}
\bibcite{chen2016xgboost}{6}
\bibcite{chen2021xgboost}{7}
\bibcite{cortes1995support}{8}
\bibcite{donoho1994ideal}{9}
\bibcite{drucker1997support}{10}
\bibcite{efron1994introduction}{11}
\bibcite{fan2001variable}{12}
\bibcite{friedman2010regularization}{13}
\bibcite{friedman2001elements}{14}
\bibcite{friedman2001greedy}{15}
\bibcite{hoerl1970ridge}{16}
\bibcite{james2013introduction}{17}
\bibcite{nielsen2016tree}{18}
\bibcite{schapire1990strength}{19}
\bibcite{schwarz1978estimating}{20}
\bibcite{tibshirani1996regression}{21}
\bibcite{zeng2017biglasso}{22}
\bibcite{zhang2010nearly}{23}
\bibcite{zou2006adaptive}{24}
\bibcite{zou2005regularization}{25}
\gdef \@abspage@last{14}
