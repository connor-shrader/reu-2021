\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{nielsen2016tree}
\citation{nielsen2016tree}
\citation{james2013introduction}
\citation{friedman2001elements}
\citation{akaike1998information}
\citation{schwarz1978estimating}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{zou2005regularization}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{breiman2001random}
\citation{chen2021xgboost}
\citation{cortes1995support}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Methodology}{4}{section.2}\protected@file@percent }
\newlabel{sec:methodology}{{2}{4}{Methodology}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Modeling Background}{4}{subsection.2.1}\protected@file@percent }
\newlabel{eqn:relationship}{{1}{4}{Modeling Background}{equation.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Linear Regression and Ordinary Least Squares}{4}{subsection.2.2}\protected@file@percent }
\newlabel{eqn:linear-model}{{2}{4}{Linear Regression and Ordinary Least Squares}{equation.2.2}{}}
\newlabel{eqn:RSS}{{4}{4}{Linear Regression and Ordinary Least Squares}{equation.2.4}{}}
\citation{friedman2001elements}
\citation{greene2003econometric}
\citation{friedman2001elements}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ols}{{1}{5}{Ordinary least squares fitting with one predictor using simulated data. The blue line represents the line found by ordinary least squares, and the red line segments are the residuals.\relax }{figure.caption.2}{}}
\newlabel{eqn:ols-solution}{{6}{5}{Linear Regression and Ordinary Least Squares}{equation.2.6}{}}
\citation{james2013introduction}
\citation{friedman2001elements}
\citation{friedman2001elements}
\citation{james2013introduction}
\citation{liu2020logsum}
\citation{sanchez2007filter}
\citation{ding2005minimum}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Subset Selection Methods}{6}{subsection.2.3}\protected@file@percent }
\citation{akaike1998information}
\citation{schwarz1978estimating}
\citation{james2013introduction}
\citation{james2013introduction}
\citation{friedman2001elements}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Penalized Regression}{8}{subsection.2.4}\protected@file@percent }
\newlabel{eqn:penalized-regression-lambda}{{9}{8}{Penalized Regression}{equation.2.9}{}}
\newlabel{eqn:penalized-regression-t}{{10}{8}{Penalized Regression}{equation.2.10}{}}
\citation{hoerl1970ridge}
\citation{tibshirani1996regression}
\citation{james2013introduction}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{zou2005regularization}
\citation{fan2001variable}
\newlabel{eqn:scad-derivative-indicator}{{12}{10}{Penalized Regression}{equation.2.12}{}}
\newlabel{fig:ridge-diagram}{{2a}{10}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:ridge-diagram}{{a}{10}{RSS contours and the ridge penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{fig:lasso-diagram}{{2b}{10}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:lasso-diagram}{{b}{10}{RSS contours and the lasso penalty boundary.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat  {\beta }^{\text  {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region in each plot represents the coefficient values selected by lasso and ridge.\relax }}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:ridge-lasso-diagram}{{2}{10}{RSS contours and penalty bounds for the ridge and lasso models when $p=2$ and $t = 1$. The red regions represent the coefficient values allowed by ridge and lasso regression, respectively. The blue ellipses represent contours of the residual sum of squares, with $\hat {\beta }^{\text {OLS}}$ being the point where the residual sum of squares is minimized. The intersection of the ellipse with the red region in each plot represents the coefficient values selected by lasso and ridge.\relax }{figure.caption.3}{}}
\citation{breheny2016lasso}
\citation{zhang2010nearly}
\citation{breheny2016lasso}
\citation{friedman2010regularization}
\citation{breheny2011ncvreg}
\newlabel{fig:penalty}{{3a}{11}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:penalty}{{a}{11}{Penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{fig:derivative}{{3b}{11}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:derivative}{{b}{11}{Derivatives of the penalty functions for lasso, SCAD, and MCP.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:lasso-scad-mcp}{{3}{11}{Penalty functions for lasso, SCAD, and MCP, as well as their derivatives. These plots use $\lambda = 1$ and $a = 3$. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.4}{}}
\citation{fan2001variable}
\citation{tibshirani1996regression}
\citation{fan2001variable}
\citation{zou2005regularization}
\citation{zhang2010nearly}
\newlabel{ols-orthonormal-solution}{{17}{12}{Penalized Regression}{equation.2.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input ($z$) represents an entry from the vector $\mathbf  {z}=\mathbf  {X}^\top \mathbf  {y}$; the output ($\hat  {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:prediction}{{4}{12}{Thresholding function for lasso, SCAD, and MCP when $\lambda = 1$ and $a = 3$ in an orthonormal design. The input ($z$) represents an entry from the vector $\mathbf {z}=\mathbf {X}^\top \mathbf {y}$; the output ($\hat {\beta }$) is the coefficient estimate. The dashed vertical lines are the knots for SCAD and MCP.\relax }{figure.caption.5}{}}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{donoho1994ideal}
\citation{zou2006adaptive}
\citation{fan2001variable}
\citation{zhang2010nearly}
\citation{zeng2017biglasso}
\citation{zeng2017biglasso}
\citation{james2013introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Non-linear models}{13}{subsection.2.5}\protected@file@percent }
\citation{breiman2001random}
\citation{efron1994introduction}
\citation{breiman1996bagging}
\citation{schapire1990strength}
\citation{friedman2001greedy}
\citation{james2013introduction}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of a decision tree. This tree was generated using a \lstinline !brTCGA! data set \cite  {zeng2017biglasso}. It attempts to predict the gene expression of the BRCA1 gene based on the expression of other genes. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }}{14}{figure.caption.6}\protected@file@percent }
\newlabel{fig:decision-tree}{{5}{14}{Example of a decision tree. This tree was generated using a \lstinline !brTCGA! data set \cite {zeng2017biglasso}. It attempts to predict the gene expression of the BRCA1 gene based on the expression of other genes. The decision tree was fitted and plotted with the \lstinline !rpart! and \lstinline !rpart.plot! libraries in \lstinline !R!.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Visualization of how predictions are made using a random forest model. An observation is input into each decision tree. Predictions from each tree are then aggregated into a single result that is used as the final prediction.\relax }}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig:random-forest}{{6}{15}{Visualization of how predictions are made using a random forest model. An observation is input into each decision tree. Predictions from each tree are then aggregated into a single result that is used as the final prediction.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces How a boosting model with decision trees is fitted. Each tree is fitted to correct the errors of the previous tree. Predictions are made by combining the results from each decision tree.\relax }}{15}{figure.caption.8}\protected@file@percent }
\newlabel{fig:boosting}{{7}{15}{How a boosting model with decision trees is fitted. Each tree is fitted to correct the errors of the previous tree. Predictions are made by combining the results from each decision tree.\relax }{figure.caption.8}{}}
\citation{chen2016xgboost}
\citation{cortes1995support}
\citation{drucker1997support}
\citation{r}
\citation{venables2002mass}
\citation{friedman2010regularization}
\citation{breheny2011ncvreg}
\citation{chen2021xgboost}
\citation{wright2017ranger}
\citation{liaw2002rf}
\citation{meyer2021e1071}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Visualization of a simple support vector machine for binary classification. The solid black line is the hyperplane that maximizes the margin between the two classes. The points with a black outline are the support vectors. They are the points that define the hyperplane.\relax }}{17}{figure.caption.9}\protected@file@percent }
\newlabel{fig:svm}{{8}{17}{Visualization of a simple support vector machine for binary classification. The solid black line is the hyperplane that maximizes the margin between the two classes. The points with a black outline are the support vectors. They are the points that define the hyperplane.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Implementation}{17}{subsection.2.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \lstinline !R! Libraries used and the models used from each library\relax }}{18}{table.caption.10}\protected@file@percent }
\newlabel{tab:model-libraries}{{1}{18}{\lstinline !R! Libraries used and the models used from each library\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Monte Carlo Simulations}{19}{section.3}\protected@file@percent }
\newlabel{sec:simulations}{{3}{19}{Monte Carlo Simulations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Simulation Design}{19}{subsection.3.1}\protected@file@percent }
\newlabel{eqn:linear-response}{{21}{20}{Simulation Design}{equation.3.21}{}}
\newlabel{eqn:nonlinear-response}{{22}{20}{Simulation Design}{equation.3.22}{}}
\newlabel{eqn:symmetric-compound-matrix}{{25}{21}{Simulation Design}{equation.3.25}{}}
\citation{liu2020logsum}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Simulation Results}{22}{subsection.3.2}\protected@file@percent }
\newlabel{sec:simulation-results}{{3.2}{22}{Simulation Results}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Average mean squared error using the training data for all linear simulations when $n = 1000$ and $p = 10$.\relax }}{23}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear-train-mse-1000-10}{{9}{23}{Average mean squared error using the training data for all linear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Average mean squared error using the test data for linear simulations when $n = 1000$ and $p = 10$.\relax }}{24}{figure.caption.12}\protected@file@percent }
\newlabel{fig:linear-test-mse-1000-10}{{10}{24}{Average mean squared error using the test data for linear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Average $\beta $-sensitivity for linear simulations when $n = 1000$ and $p = 10$.\relax }}{25}{figure.caption.13}\protected@file@percent }
\newlabel{fig:linear-sensitivity-1000-10}{{11}{25}{Average $\beta $-sensitivity for linear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Average $\beta $-specificity for linear simulations when $n = 1000$ and $p = 10$.\relax }}{25}{figure.caption.13}\protected@file@percent }
\newlabel{fig:linear-specificity-1000-10}{{12}{25}{Average $\beta $-specificity for linear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Average mean square error using training data for linear simulations when $n = 50$ and $p = 2000$.\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{fig:linear-linear-train-mse-50-2000}{{13}{26}{Average mean square error using training data for linear simulations when $n = 50$ and $p = 2000$.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Average mean square error using testing data for linear simulations when $n = 50$ and $p = 2000$.\relax }}{26}{figure.caption.14}\protected@file@percent }
\newlabel{fig:linear-linear-test-mse-50-2000}{{14}{26}{Average mean square error using testing data for linear simulations when $n = 50$ and $p = 2000$.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Average $\beta $-sensitivity for linear simulations when $n = 50$ and $p = 2000$\relax }}{27}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linear-linear-sensitivity-50-2000}{{15}{27}{Average $\beta $-sensitivity for linear simulations when $n = 50$ and $p = 2000$\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Average $\beta $-specificity for linear simulations when $n = 50$ and $p = 2000$.\relax }}{27}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linear-linear-specificity-50-2000}{{16}{27}{Average $\beta $-specificity for linear simulations when $n = 50$ and $p = 2000$.\relax }{figure.caption.15}{}}
\citation{zeng2017biglasso}
\citation{kuchenbaecker2017risks}
\citation{antoniou2003average}
\citation{deng2000roles}
\citation{deng2000roles}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Nonlinear Simulation Results}{28}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Empirical Data Analysis}{28}{section.4}\protected@file@percent }
\newlabel{sec:empirical}{{4}{28}{Empirical Data Analysis}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Details of Empirical Data}{28}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Average mean square error using training data for nonlinear simulations when $n = 1000$ and $p = 10$.\relax }}{29}{figure.caption.16}\protected@file@percent }
\newlabel{fig:nonlinear-train-mse-1000-10}{{17}{29}{Average mean square error using training data for nonlinear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Average mean square error using training data for nonlinear simulations when $n = 1000$ and $p = 10$.\relax }}{29}{figure.caption.17}\protected@file@percent }
\newlabel{fig:nonlinear-test-mse-1000-10}{{18}{29}{Average mean square error using training data for nonlinear simulations when $n = 1000$ and $p = 10$.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Average mean square error using training data for nonlinear simulations when $n = 50$ and $p = 2000$.\relax }}{30}{figure.caption.18}\protected@file@percent }
\newlabel{fig:nonlinear-train-mse-50-2000}{{19}{30}{Average mean square error using training data for nonlinear simulations when $n = 50$ and $p = 2000$.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Average mean square error using training data for nonlinear simulations when $n = 50$ and $p = 2000$.\relax }}{30}{figure.caption.19}\protected@file@percent }
\newlabel{fig:nonlinear-test-mse-50-2000}{{20}{30}{Average mean square error using training data for nonlinear simulations when $n = 50$ and $p = 2000$.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Distribution of BRCA1 gene expression levels. This is the variable that we used as the response. The mean gene expression is -1.459.\relax }}{31}{figure.caption.20}\protected@file@percent }
\newlabel{fig:BRCA1-distribution}{{21}{31}{Distribution of BRCA1 gene expression levels. This is the variable that we used as the response. The mean gene expression is -1.459.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Empirical Data Results}{32}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }}{32}{figure.caption.21}\protected@file@percent }
\newlabel{fig:empirical_mse}{{22}{32}{Mean squared error of the models fit on the bcTCGA data set. Each point represents the mean squared error for one fold, while the bars represent the average for the five folds.\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }}{32}{table.caption.22}\protected@file@percent }
\newlabel{tab:emp_results}{{2}{32}{Average and standard deviation of the mean squared error of the models fit on the bcTCGA data set.\relax }{table.caption.22}{}}
\citation{xiao2016nbr2}
\citation{kobayashi2015overexpression}
\citation{vaccari2005drosophila}
\citation{shao2007cmtm5}
\newlabel{fig:venn1}{{23a}{33}{First fold\relax }{figure.caption.23}{}}
\newlabel{sub@fig:venn1}{{a}{33}{First fold\relax }{figure.caption.23}{}}
\newlabel{fig:venn2}{{23b}{33}{Second fold\relax }{figure.caption.23}{}}
\newlabel{sub@fig:venn2}{{b}{33}{Second fold\relax }{figure.caption.23}{}}
\newlabel{fig:venn3}{{23c}{33}{Third fold\relax }{figure.caption.23}{}}
\newlabel{sub@fig:venn3}{{c}{33}{Third fold\relax }{figure.caption.23}{}}
\newlabel{fig:venn4}{{23d}{33}{Fourth Fold\relax }{figure.caption.23}{}}
\newlabel{sub@fig:venn4}{{d}{33}{Fourth Fold\relax }{figure.caption.23}{}}
\newlabel{fig:venn5}{{23e}{33}{Fifth Fold\relax }{figure.caption.23}{}}
\newlabel{sub@fig:venn5}{{e}{33}{Fifth Fold\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Venn diagrams of the predictors selected by lasso, elastic-net, MCP, and random forest models for each of the five folds. Each number represents the number of important predictors chosen by the models that overlap the number.\relax }}{33}{figure.caption.23}\protected@file@percent }
\newlabel{fig:venn}{{23}{33}{Venn diagrams of the predictors selected by lasso, elastic-net, MCP, and random forest models for each of the five folds. Each number represents the number of important predictors chosen by the models that overlap the number.\relax }{figure.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Average Runtimes for Empirical Data Models\relax }}{34}{table.caption.24}\protected@file@percent }
\newlabel{tab:emp_runtimes}{{3}{34}{Average Runtimes for Empirical Data Models\relax }{table.caption.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{35}{section.5}\protected@file@percent }
\newlabel{sec:discussion}{{5}{35}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Discussion of Monte Carlo Results}{35}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Discussion of Empirical Data Results}{37}{subsection.5.2}\protected@file@percent }
\citation{capitaine2021random}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{38}{section.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{38}{Conclusion}{section.6}{}}
\newlabel{eqn:proposed}{{31}{38}{Conclusion}{equation.6.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Acknowledgments}{39}{section.7}\protected@file@percent }
\bibstyle{plain-lf}
\bibdata{references}
\bibcite{akaike1998information}{1}
\bibcite{antoniou2003average}{2}
\bibcite{breheny2016lasso}{3}
\bibcite{breheny2011ncvreg}{4}
\bibcite{breiman1996bagging}{5}
\bibcite{breiman2001random}{6}
\bibcite{capitaine2021random}{7}
\bibcite{chen2016xgboost}{8}
\bibcite{chen2021xgboost}{9}
\bibcite{cortes1995support}{10}
\bibcite{deng2000roles}{11}
\bibcite{ding2005minimum}{12}
\bibcite{donoho1994ideal}{13}
\bibcite{drucker1997support}{14}
\bibcite{efron1994introduction}{15}
\bibcite{fan2001variable}{16}
\bibcite{friedman2010regularization}{17}
\bibcite{friedman2001elements}{18}
\bibcite{friedman2001greedy}{19}
\bibcite{greene2003econometric}{20}
\bibcite{hoerl1970ridge}{21}
\bibcite{james2013introduction}{22}
\bibcite{kobayashi2015overexpression}{23}
\bibcite{kuchenbaecker2017risks}{24}
\bibcite{liaw2002rf}{25}
\bibcite{liu2020logsum}{26}
\bibcite{meyer2021e1071}{27}
\bibcite{nielsen2016tree}{28}
\bibcite{r}{29}
\bibcite{sanchez2007filter}{30}
\bibcite{schapire1990strength}{31}
\bibcite{schwarz1978estimating}{32}
\bibcite{shao2007cmtm5}{33}
\bibcite{tibshirani1996regression}{34}
\bibcite{vaccari2005drosophila}{35}
\bibcite{venables2002mass}{36}
\bibcite{wright2017ranger}{37}
\bibcite{xiao2016nbr2}{38}
\bibcite{zeng2017biglasso}{39}
\bibcite{zhang2010nearly}{40}
\bibcite{zou2006adaptive}{41}
\bibcite{zou2005regularization}{42}
\@writefile{toc}{\contentsline {section}{\numberline {A}Full Result Tables}{43}{appendix.A}\protected@file@percent }
\newlabel{app:full-results}{{A}{43}{Full Result Tables}{appendix.A}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$. See Figure \ref  {fig:linear-train-mse-1000-10} for the corresponding plot.\relax }}{44}{table.4}\protected@file@percent }
\newlabel{tab:train-mse-1000-10}{{4}{44}{Mean and standard deviation of the mean squared error on training data when $n = 1000$ and $p = 10$. See Figure \ref {fig:linear-train-mse-1000-10} for the corresponding plot.\relax }{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Mean and standard deviation of the mean squared error on test data when $n = 1000$ and $p = 10$. See Figure \ref  {fig:linear-test-mse-1000-10} for the corresponding plot.\relax }}{45}{table.5}\protected@file@percent }
\newlabel{tab:test-mse-1000-10}{{5}{45}{Mean and standard deviation of the mean squared error on test data when $n = 1000$ and $p = 10$. See Figure \ref {fig:linear-test-mse-1000-10} for the corresponding plot.\relax }{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Mean and standard deviation of the $\beta $-sensitivity when $n = 1000$ and $p = 10$. See Figure \ref  {fig:linear-sensitivity-1000-10} for the corresponding plot.\relax }}{46}{table.6}\protected@file@percent }
\newlabel{tab:sensitivity-1000-10}{{6}{46}{Mean and standard deviation of the $\beta $-sensitivity when $n = 1000$ and $p = 10$. See Figure \ref {fig:linear-sensitivity-1000-10} for the corresponding plot.\relax }{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Mean and standard deviation of the $\beta $-specificity when $n = 1000$ and $p = 10$. See Figure \ref  {fig:linear-specificity-1000-10} for the corresponding plot.\relax }}{47}{table.7}\protected@file@percent }
\newlabel{tab:specificity-1000-10}{{7}{47}{Mean and standard deviation of the $\beta $-specificity when $n = 1000$ and $p = 10$. See Figure \ref {fig:linear-specificity-1000-10} for the corresponding plot.\relax }{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 50$ and $p = 2000$. See Figure \ref  {fig:linear-train-mse-50-2000} for the corresponding plot.\relax }}{48}{table.8}\protected@file@percent }
\newlabel{tab:train-mse-50-2000}{{8}{48}{Mean and standard deviation of the mean squared error on training data when $n = 50$ and $p = 2000$. See Figure \ref {fig:linear-train-mse-50-2000} for the corresponding plot.\relax }{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Mean and standard deviation of the mean squared error on training data when $n = 50$ and $p = 2000$. See Figure \ref  {fig:linear-test-mse-50-2000} for the corresponding plot.\relax }}{49}{table.9}\protected@file@percent }
\newlabel{tab:test-mse-50-2000}{{9}{49}{Mean and standard deviation of the mean squared error on training data when $n = 50$ and $p = 2000$. See Figure \ref {fig:linear-test-mse-50-2000} for the corresponding plot.\relax }{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Mean and standard deviation of the $\beta $-sensitivity when $n = 50$ and $p = 2000$. See Figure \ref  {fig:linear-sensitivity-50-2000} for the corresponding plot.\relax }}{50}{table.10}\protected@file@percent }
\newlabel{tab:sensitivity-50-2000}{{10}{50}{Mean and standard deviation of the $\beta $-sensitivity when $n = 50$ and $p = 2000$. See Figure \ref {fig:linear-sensitivity-50-2000} for the corresponding plot.\relax }{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {11}{\ignorespaces Mean and standard deviation of the $\beta $-specificity when $n = 50$ and $p = 2000$. See Figure \ref  {fig:linear-specificity-50-2000} for the corresponding plot.\relax }}{50}{table.11}\protected@file@percent }
\newlabel{tab:specificity-50-2000}{{11}{50}{Mean and standard deviation of the $\beta $-specificity when $n = 50$ and $p = 2000$. See Figure \ref {fig:linear-specificity-50-2000} for the corresponding plot.\relax }{table.11}{}}
\gdef \@abspage@last{50}
